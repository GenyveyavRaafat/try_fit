{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!export TORCH_CUDA_ARCH_LIST=\"6.1;7.5;8.0\"\n"
      ],
      "metadata": {
        "id": "_Od-NUPOI2wt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXSlN0QrifUh",
        "outputId": "2af6bc60-11d7-47f5-ed9e-075d3bc9380c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.6.15)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.1)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.11-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: firebase-admin in /usr/local/lib/python3.11/dist-packages (6.9.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: cachecontrol>=0.12.14 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (0.14.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.7.8 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.173.0)\n",
            "Requirement already satisfied: google-cloud-storage>=1.37.1 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.19.0)\n",
            "Requirement already satisfied: pyjwt>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (2.10.1)\n",
            "Requirement already satisfied: httpx==0.28.1 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]==0.28.1->firebase-admin) (0.28.1)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=1.22.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.25.1)\n",
            "Requirement already satisfied: google-cloud-firestore>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from firebase-admin) (2.21.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (3.10)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]==0.28.1->firebase-admin) (4.2.0)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from cachecontrol>=0.12.14->firebase-admin) (1.1.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (1.71.0)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (4.2.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-firestore>=2.19.0->firebase-admin) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (2.7.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage>=1.37.1->firebase-admin) (1.7.1)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.11/dist-packages (from pyjwt[crypto]>=2.5.0->firebase-admin) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (4.9.1)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]==0.28.1->firebase-admin) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]==0.28.1->firebase-admin) (4.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.7.8->firebase-admin) (3.2.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx==0.28.1->httpx[http2]==0.28.1->firebase-admin) (4.14.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4.0->pyjwt[crypto]>=2.5.0->firebase-admin) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0dev,>=1.22.1->google-api-core[grpc]<3.0.0dev,>=1.22.1; platform_python_implementation != \"PyPy\"->firebase-admin) (0.6.1)\n",
            "Downloading pyngrok-7.2.11-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.11\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\n",
            "added 22 packages in 2s\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0KCollecting cloudinary\n",
            "  Downloading cloudinary-1.44.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from cloudinary) (1.17.0)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in /usr/local/lib/python3.11/dist-packages (from cloudinary) (2.4.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from cloudinary) (2025.6.15)\n",
            "Downloading cloudinary-1.44.1-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: cloudinary\n",
            "Successfully installed cloudinary-1.44.1\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install flask pyngrok firebase-admin pillow requests\n",
        "!npm install -g localtunnel\n",
        "!pip install cloudinary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zLhFKME6DIyq",
        "outputId": "bddb83a4-02c9-4123-ef75-575acd6905dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.0.1\n",
            "  Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl.metadata (24 kB)\n",
            "Collecting torchvision==0.15.2\n",
            "  Downloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl.metadata (11 kB)\n",
            "Collecting opencv-python==4.7.0.72\n",
            "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting diffusers==0.14.0\n",
            "  Downloading diffusers-0.14.0-py3-none-any.whl.metadata (32 kB)\n",
            "Collecting transformers==4.27.3\n",
            "  Downloading transformers-4.27.3-py3-none-any.whl.metadata (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.18.0\n",
            "  Downloading accelerate-0.18.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting clean-fid==0.1.35\n",
            "  Downloading clean_fid-0.1.35-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting torchmetrics==0.11.4 (from torchmetrics[image]==0.11.4)\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting wandb==0.14.0\n",
            "  Downloading wandb-0.14.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Collecting matplotlib==3.7.1\n",
            "  Downloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.31-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.7.101 (from torch==2.0.1)\n",
            "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch==2.0.1)\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch==2.0.1)\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.0.1)\n",
            "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.2.10.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.0.1 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.4.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu11==2.14.3 (from torch==2.0.1)\n",
            "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.7.91 (from torch==2.0.1)\n",
            "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.0.0 (from torch==2.0.1)\n",
            "  Downloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.15.2) (11.2.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.14.0) (8.7.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.14.0) (0.33.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.14.0) (2024.11.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.3) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.27.3) (6.0.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.3)\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate==0.18.0) (5.9.5)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from clean-fid==0.1.35) (1.15.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.14.0) (8.2.1)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.14.0) (3.1.44)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb==0.14.0) (2.31.0)\n",
            "Collecting docker-pycreds>=0.4.0 (from wandb==0.14.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting pathtools (from wandb==0.14.0)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb==0.14.0) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb==0.14.0) (75.2.0)\n",
            "Collecting appdirs>=1.4.3 (from wandb==0.14.0)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting protobuf!=4.21.0,<5,>=3.19.0 (from wandb==0.14.0)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.7.1) (2.9.0.post0)\n",
            "Collecting lpips<=0.1.4 (from torchmetrics[image]==0.11.4)\n",
            "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting torch-fidelity<=0.3.0 (from torchmetrics[image]==0.11.4)\n",
            "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Collecting lit (from triton==2.0.0->torch==2.0.1)\n",
            "  Downloading lit-18.1.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "INFO: pip is looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.29.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.29.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.29.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.29-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.28.post3-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.28.post2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "INFO: pip is still looking at multiple versions of xformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading xformers-0.0.28.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.28-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.27.post2-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.27.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.27-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading xformers-0.0.26.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.25.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.25-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.24-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.23.post1-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.23-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.22.post7-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "  Downloading xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb==0.14.0) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.14.0) (4.0.12)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.14.0) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->diffusers==0.14.0) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.15.2) (2025.6.15)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers==0.14.0) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.14.0) (5.0.2)\n",
            "Downloading torch-2.0.1-cp311-cp311-manylinux1_x86_64.whl (619.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.15.2-cp311-cp311-manylinux1_x86_64.whl (6.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diffusers-0.14.0-py3-none-any.whl (737 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.27.3-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-0.18.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.3/215.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading clean_fid-0.1.35-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wandb-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m109.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.22-cp311-cp311-manylinux2014_x86_64.whl (211.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.6/211.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m125.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
            "Downloading lit-18.1.8-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.4/96.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8792 sha256=50de582dac9e92ad03300e1248e60d48626225c2edc50487b4dacbaea143cec2\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/b7/8b/84e94095ea418b9442f5abeba4ca7b0ad52d3fe7b69d6238a6\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, lit, appdirs, protobuf, opencv-python, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, docker-pycreds, nvidia-cusolver-cu11, nvidia-cudnn-cu11, matplotlib, wandb, transformers, diffusers, triton, torch, torchvision, torchmetrics, torch-fidelity, lpips, xformers, clean-fid, accelerate\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.11.0.86\n",
            "    Uninstalling opencv-python-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-4.11.0.86\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: wandb\n",
            "    Found existing installation: wandb 0.20.1\n",
            "    Uninstalling wandb-0.20.1:\n",
            "      Successfully uninstalled wandb-0.20.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.4\n",
            "    Uninstalling transformers-4.52.4:\n",
            "      Successfully uninstalled transformers-4.52.4\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.34.0\n",
            "    Uninstalling diffusers-0.34.0:\n",
            "      Successfully uninstalled diffusers-0.34.0\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.8.1\n",
            "    Uninstalling accelerate-1.8.1:\n",
            "      Successfully uninstalled accelerate-1.8.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires accelerate>=0.21.0, but you have accelerate 0.18.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.0.1 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.18.0 appdirs-1.4.4 clean-fid-0.1.35 diffusers-0.14.0 docker-pycreds-0.4.0 lit-18.1.8 lpips-0.1.4 matplotlib-3.7.1 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 opencv-python-4.7.0.72 pathtools-0.1.2 protobuf-4.25.8 tokenizers-0.13.3 torch-2.0.1 torch-fidelity-0.3.0 torchmetrics-0.11.4 torchvision-0.15.2 transformers-4.27.3 triton-2.0.0 wandb-0.14.0 xformers-0.0.22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "matplotlib",
                  "mpl_toolkits"
                ]
              },
              "id": "e4a37e85ed57400c86484f1e789f82af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub==0.16.4\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (3.18.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (4.67.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (4.14.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.16.4) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.16.4) (2025.6.15)\n",
            "Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.33.0\n",
            "    Uninstalling huggingface-hub-0.33.0:\n",
            "      Successfully uninstalled huggingface-hub-0.33.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires accelerate>=0.21.0, but you have accelerate 0.18.0 which is incompatible.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "gradio-client 1.10.1 requires huggingface-hub>=0.19.3, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface_hub-0.16.4\n",
            "Collecting jax==0.4.14\n",
            "  Downloading jax-0.4.14.tar.gz (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jaxlib==0.4.14\n",
            "  Downloading jaxlib-0.4.14-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting flax==0.7.2\n",
            "  Downloading flax-0.7.2-py3-none-any.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.14) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.14) (2.0.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax==0.4.14) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.14) (1.15.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (0.11.16)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (0.1.74)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (4.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (6.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax==0.7.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax==0.7.2) (2.19.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax->flax==0.7.2) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax==0.7.2) (0.1.89)\n",
            "INFO: pip is looking at multiple versions of optax to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting optax (from flax==0.7.2)\n",
            "  Downloading optax-0.2.4-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading optax-0.2.3-py3-none-any.whl.metadata (8.3 kB)\n",
            "  Downloading optax-0.2.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.7.2) (1.12.2)\n",
            "INFO: pip is looking at multiple versions of orbax-checkpoint to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting orbax-checkpoint (from flax==0.7.2)\n",
            "  Downloading orbax_checkpoint-0.11.15-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.14-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.13-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.12-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.11-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.10-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.9-py3-none-any.whl.metadata (2.0 kB)\n",
            "INFO: pip is still looking at multiple versions of orbax-checkpoint to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading orbax_checkpoint-0.11.8-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.7-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.11.6-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.11.5-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.11.4-py3-none-any.whl.metadata (1.9 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading orbax_checkpoint-0.11.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.11.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.11.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.10.3-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.10.2-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.10.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.10.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "  Downloading orbax_checkpoint-0.9.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.9.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.8.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.7.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.6.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.6.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.6.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.6.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.6.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.23-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.22-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.21-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.19-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.18-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.17-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.16-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.7.2) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.7.2) (4.25.8)\n",
            "INFO: pip is looking at multiple versions of chex to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting chex>=0.1.86 (from optax->flax==0.7.2)\n",
            "  Downloading chex-0.1.88-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading chex-0.1.87-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading chex-0.1.86-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting orbax-checkpoint (from flax==0.7.2)\n",
            "  Downloading orbax_checkpoint-0.5.15-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: pip is still looking at multiple versions of chex to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading orbax_checkpoint-0.5.14-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.13-py3-none-any.whl.metadata (1.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading orbax_checkpoint-0.5.12-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.11-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.10-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.5.9-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.6-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.5-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.5.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.8-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.6-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.5-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.3-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.1-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.4.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.3.5-py3-none-any.whl.metadata (1.7 kB)\n",
            "  Downloading orbax_checkpoint-0.3.4-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.3.3-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.3.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.3.1-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.2.7-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.2.6-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting cached_property (from orbax-checkpoint->flax==0.7.2)\n",
            "  Downloading cached_property-2.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax==0.7.2) (6.5.2)\n",
            "Collecting orbax-checkpoint (from flax==0.7.2)\n",
            "  Downloading orbax_checkpoint-0.2.5-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.2.4-py3-none-any.whl.metadata (2.0 kB)\n",
            "  Downloading orbax_checkpoint-0.2.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.2.1-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading orbax_checkpoint-0.1.8-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading orbax_checkpoint-0.1.7-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading orbax_checkpoint-0.1.6-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading orbax_checkpoint-0.1.4-py3-none-any.whl.metadata (1.3 kB)\n",
            "  Downloading orbax_checkpoint-0.1.1-py3-none-any.whl.metadata (1.2 kB)\n",
            "  Downloading orbax_checkpoint-0.0.0-py3-none-any.whl.metadata (435 bytes)\n",
            "Collecting optax (from flax==0.7.2)\n",
            "  Downloading optax-0.2.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting chex>=0.1.7 (from optax->flax==0.7.2)\n",
            "  Downloading chex-0.1.85-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading chex-0.1.84-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading chex-0.1.83-py3-none-any.whl.metadata (17 kB)\n",
            "  Downloading chex-0.1.82-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.7->optax->flax==0.7.2) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.7.2) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.7.2) (2025.3.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.7.2) (3.23.0)\n",
            "Downloading jaxlib-0.4.14-cp311-cp311-manylinux2014_x86_64.whl (73.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.7/73.7 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flax-0.7.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.6/226.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading optax-0.2.1-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.9/209.9 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chex-0.1.82-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orbax_checkpoint-0.5.16-py3-none-any.whl (217 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m217.0/217.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: jax\n",
            "  Building wheel for jax (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.4.14-py3-none-any.whl size=1535471 sha256=193558fea354038eab06f3296773aafbc61d08f8d14a06f06bc26d4af8659f55\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/8d/5d/66b1fbb551b0c3a21696015b7339b8241ebfa128bb9145febd\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax, orbax-checkpoint, chex, optax, flax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "  Attempting uninstall: orbax-checkpoint\n",
            "    Found existing installation: orbax-checkpoint 0.11.16\n",
            "    Uninstalling orbax-checkpoint-0.11.16:\n",
            "      Successfully uninstalled orbax-checkpoint-0.11.16\n",
            "  Attempting uninstall: chex\n",
            "    Found existing installation: chex 0.1.89\n",
            "    Uninstalling chex-0.1.89:\n",
            "      Successfully uninstalled chex-0.1.89\n",
            "  Attempting uninstall: optax\n",
            "    Found existing installation: optax 0.2.5\n",
            "    Uninstalling optax-0.2.5:\n",
            "      Successfully uninstalled optax-0.2.5\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.10.6\n",
            "    Uninstalling flax-0.10.6:\n",
            "      Successfully uninstalled flax-0.10.6\n",
            "Successfully installed chex-0.1.82 flax-0.7.2 jax-0.4.14 jaxlib-0.4.14 optax-0.2.1 orbax-checkpoint-0.5.16\n",
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "peft 0.15.2 requires accelerate>=0.21.0, but you have accelerate 0.18.0 which is incompatible.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires huggingface-hub>=0.20.0, but you have huggingface-hub 0.16.4 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.27.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "14de1713a0284ffbac642705be8a8e15"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# don't restart the cell until it finishes\n",
        "!pip install torch==2.0.1 torchvision==0.15.2 opencv-python==4.7.0.72 diffusers==0.14.0 \\\n",
        "    transformers==4.27.3 accelerate==0.18.0 clean-fid==0.1.35 \"torchmetrics[image]==0.11.4\" \\\n",
        "    wandb==0.14.0 matplotlib==3.7.1 tqdm xformers\n",
        "!pip install huggingface_hub==0.16.4\n",
        "!pip install jax==0.4.14 jaxlib==0.4.14 flax==0.7.2\n",
        "!pip install numpy==1.26.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q375uEetj7vq",
        "outputId": "0e8a05a2-cfce-4859-ddad-428e0da5ad6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x7B0PWtrU3K"
      },
      "outputs": [],
      "source": [
        "def check_category(cloth_url):\n",
        "\n",
        "    category_query = db.collection(\"categories\").where(\"image_url\", \"==\", cloth_url).limit(1).stream()\n",
        "    category_doc = next(category_query, None)\n",
        "\n",
        "    if not category_doc:\n",
        "        raise ValueError(\"Category not found for the cloth image\")\n",
        "\n",
        "    category_data = category_doc.to_dict()\n",
        "    category_name = category_data.get('category_name', \"Unknown Category\")\n",
        "\n",
        "    # Ensure the category is valid\n",
        "    if category_name not in ['Upper', 'Lower', 'Full']:\n",
        "        raise ValueError(f\"Invalid category: {category_name}\")\n",
        "\n",
        "    return category_name\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv_mp6BbE4Iy",
        "outputId": "d5fdfded-ee23-4503-9daa-ded853d8d313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch-openpose\n",
            "🔧 Installing system & Python dependencies...\n",
            "📁 Cloning pytorch-openpose repo...\n",
            "⬇ Downloading OpenPose model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1UduPqS9IJdfhMbvQkKybG3ym99Ag788S\n",
            "From (redirected): https://drive.google.com/uc?id=1UduPqS9IJdfhMbvQkKybG3ym99Ag788S&confirm=t&uuid=75b74095-ede4-41c2-9bbe-eb311f84e4dc\n",
            "To: /content/pytorch-openpose/model/body_pose_model.pth\n",
            "100%|██████████| 209M/209M [00:03<00:00, 62.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇ Downloading inference script...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1N2B_xindDuLkvFulu5DMJeMYjtunFhLt\n",
            "From (redirected): https://drive.google.com/uc?id=1N2B_xindDuLkvFulu5DMJeMYjtunFhLt&confirm=t&uuid=c97e0496-47a6-4880-991b-13e4d2992675\n",
            "To: /content/pytorch-openpose/inference_keypoints.py\n",
            "100%|██████████| 2.91k/2.91k [00:00<00:00, 4.66MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "import subprocess\n",
        "import gdown\n",
        "import numpy as np\n",
        "\n",
        "REPO_DIR = os.path.abspath(\"pytorch-openpose\")\n",
        "print(REPO_DIR)\n",
        "MODEL_PATH = os.path.join(REPO_DIR, \"model\", \"body_pose_model.pth\")\n",
        "INFERENCE_PATH = os.path.join(REPO_DIR, \"inference_keypoints.py\")\n",
        "\n",
        "\n",
        "def setup_openpose_environment():\n",
        "    print(\"🔧 Installing system & Python dependencies...\")\n",
        "\n",
        "    # Install system dependencies\n",
        "    subprocess.run([\"apt-get\", \"update\", \"-y\"], check=True, capture_output=True)\n",
        "    subprocess.run([\n",
        "        \"apt-get\", \"install\", \"-y\",\n",
        "        \"build-essential\", \"libssl-dev\", \"libffi-dev\", \"libbz2-dev\",\n",
        "        \"libreadline-dev\", \"libsqlite3-dev\", \"libncurses5-dev\", \"libncursesw5-dev\",\n",
        "        \"xz-utils\", \"tk-dev\", \"liblzma-dev\", \"python3-openssl\", \"git\",\n",
        "        \"libc6-dev\", \"libgomp1\", \"libgcc-s1\", \"g++\"\n",
        "    ], check=True, capture_output=True)\n",
        "\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"torch\", \"torchvision\", \"opencv-python-headless\",\n",
        "                    \"matplotlib\", \"gdown\", \"tqdm\", \"scipy\", \"onnx\", \"onnxruntime\"], check=True)\n",
        "\n",
        "    if not os.path.exists(REPO_DIR):\n",
        "        print(\"📁 Cloning pytorch-openpose repo...\")\n",
        "        subprocess.run([\n",
        "            \"git\", \"clone\", \"-q\",\n",
        "            \"https://github.com/Hzzone/pytorch-openpose.git\"\n",
        "        ], check=True, capture_output=True)\n",
        "\n",
        "    os.makedirs(os.path.join(REPO_DIR, \"inputs\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(REPO_DIR, \"skeleton_results\"), exist_ok=True)\n",
        "    os.makedirs(os.path.join(REPO_DIR, \"keypoints_results\"), exist_ok=True)\n",
        "\n",
        "    # Download model checkpoint\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        print(\"⬇ Downloading OpenPose model...\")\n",
        "        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "        gdown.download(\n",
        "            \"https://drive.google.com/uc?id=1UduPqS9IJdfhMbvQkKybG3ym99Ag788S\",\n",
        "            MODEL_PATH,\n",
        "            quiet=False\n",
        "        )\n",
        "\n",
        "    if not os.path.exists(INFERENCE_PATH):\n",
        "        print(\"⬇ Downloading inference script...\")\n",
        "        gdown.download(\n",
        "            id=\"1N2B_xindDuLkvFulu5DMJeMYjtunFhLt\",\n",
        "            output=INFERENCE_PATH,\n",
        "            quiet=False\n",
        "        )\n",
        "\n",
        "setup_openpose_environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ndbQKKyEaz",
        "outputId": "5597f98c-24c7-457b-9bbd-f0317cb62cdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚙️ Setting up environment...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP\n",
            "From (redirected): https://drive.google.com/uc?id=1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP&confirm=t&uuid=ad4a1edb-fadb-49a4-9bca-00149bfea43c\n",
            "To: /content/Self-Correction-Human-Parsing/checkpoints/final.pth\n",
            "100%|██████████| 267M/267M [00:02<00:00, 96.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Environment setup completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import gdown\n",
        "import cv2\n",
        "\n",
        "def setup_environment():\n",
        "    marker_file = \".setup_done\"\n",
        "    if os.path.exists(marker_file):\n",
        "        print(\"✅ Environment already set up. Skipping setup.\")\n",
        "        return\n",
        "\n",
        "    print(\"⚙️ Setting up environment...\")\n",
        "\n",
        "    # Install system dependencies\n",
        "    subprocess.run([\n",
        "        \"apt-get\", \"update\", \"-y\"\n",
        "    ], check=True)\n",
        "    subprocess.run([\n",
        "        \"apt-get\", \"install\", \"-y\",\n",
        "        \"wget\", \"build-essential\", \"libssl-dev\", \"libffi-dev\", \"libbz2-dev\",\n",
        "        \"libreadline-dev\", \"libsqlite3-dev\", \"libncurses5-dev\", \"libncursesw5-dev\",\n",
        "        \"xz-utils\", \"tk-dev\", \"liblzma-dev\", \"python3-openssl\", \"git\",\n",
        "        \"libc6-dev\", \"libgomp1\", \"libgcc-s1\", \"g++\", \"curl\", \"zlib1g-dev\",\n",
        "        \"uuid-dev\", \"libnss3-dev\"\n",
        "    ], check=True)\n",
        "\n",
        "    # Download and build Python 3.8\n",
        "    subprocess.run([\"wget\", \"https://www.python.org/ftp/python/3.8.18/Python-3.8.18.tgz\"], check=True)\n",
        "    subprocess.run([\"tar\", \"-xvf\", \"Python-3.8.18.tgz\"], check=True)\n",
        "    os.chdir(\"Python-3.8.18\")\n",
        "    subprocess.run([\"./configure\", \"--enable-optimizations\"], check=True)\n",
        "    subprocess.run([\"make\", \"-j\", \"2\"], check=True)\n",
        "    subprocess.run([\"make\", \"altinstall\"], check=True)\n",
        "    os.chdir(\"..\")\n",
        "\n",
        "    # Install pip using ensurepip\n",
        "    subprocess.run([\"/usr/local/bin/python3.8\", \"-m\", \"ensurepip\"], check=True)\n",
        "    subprocess.run([\"/usr/local/bin/python3.8\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"], check=True)\n",
        "\n",
        "    # Install Python dependencies\n",
        "    subprocess.run([\n",
        "        \"/usr/local/bin/python3.8\", \"-m\", \"pip\", \"install\",\n",
        "        \"ninja\", \"torch\", \"torchvision\", \"gdown\", \"tqdm\", \"opencv-python-headless\"\n",
        "    ], check=True)\n",
        "\n",
        "    repo_dir = \"Self-Correction-Human-Parsing\"\n",
        "    if not os.path.exists(repo_dir):\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/PeikeLi/Self-Correction-Human-Parsing\"], check=True)\n",
        "\n",
        "    os.makedirs(f\"{repo_dir}/checkpoints\", exist_ok=True)\n",
        "    os.makedirs(f\"{repo_dir}/inputs\", exist_ok=True)\n",
        "    os.makedirs(f\"{repo_dir}/outputs\", exist_ok=True)\n",
        "\n",
        "    # Download model checkpoint\n",
        "    model_path = f\"{repo_dir}/checkpoints/final.pth\"\n",
        "    if not os.path.exists(model_path):\n",
        "        gdown.download(\n",
        "            \"https://drive.google.com/uc?id=1ruJg4lqR_jgQPj-9K0PP-L2vJERYOxLP\",\n",
        "            model_path,\n",
        "            quiet=False\n",
        "        )\n",
        "\n",
        "    with open(marker_file, \"w\") as f:\n",
        "        f.write(\"setup done\")\n",
        "\n",
        "    print(\"✅ Environment setup completed.\")\n",
        "\n",
        "\n",
        "setup_environment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pt-6D_IQyRoK"
      },
      "outputs": [],
      "source": [
        "def extract_keypoints_skeletons(input_image_path, output_json_path, output_image_path):\n",
        "    if not os.path.exists(REPO_DIR) or not os.path.exists(MODEL_PATH) or not os.path.exists(INFERENCE_PATH):\n",
        "        print(\"⚙ First-time setup required.\")\n",
        "        setup_openpose_environment()\n",
        "    else:\n",
        "        print(\"🔁 Environment already set up. Skipping setup.\")\n",
        "\n",
        "    input_filename = os.path.basename(input_image_path)\n",
        "    input_path_in_repo = os.path.join(REPO_DIR, \"inputs\", input_filename)\n",
        "\n",
        "    img = cv2.imread(input_image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Invalid image file: {input_image_path}\")\n",
        "    cv2.imwrite(input_path_in_repo, img)\n",
        "    print(f\"📷 Saved input image to: {input_path_in_repo}\")\n",
        "\n",
        "    # Run inference script\n",
        "    print(\"🔍 Running keypoint detection...\")\n",
        "    cmd = (\n",
        "        f\"cd {REPO_DIR} && \"\n",
        "        f\"python3 inference_keypoints.py \"\n",
        "        f\"--input {input_path_in_repo} \"\n",
        "        f\"--output_json keypoints_results \"\n",
        "        f\"--output_image skeleton_results \"\n",
        "    )\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(\"❌ Inference failed!\")\n",
        "        print(result.stderr)\n",
        "        raise RuntimeError(\"Keypoint inference script failed.\")\n",
        "    else:\n",
        "        print(\"✅ Inference successful.\")\n",
        "        print(result.stdout)\n",
        "\n",
        "    # Move outputs to desired paths\n",
        "    base_name = os.path.splitext(input_filename)[0].replace(\"_0\", \"\")\n",
        "    generated_json = os.path.join(REPO_DIR, \"keypoints_results\", f\"{base_name}_2.json\")\n",
        "    generated_image_jpg = os.path.join(REPO_DIR, \"skeleton_results\", f\"{base_name}_5.png\")\n",
        "\n",
        "    if not os.path.exists(generated_json):\n",
        "        raise FileNotFoundError(f\"Keypoint JSON not found at {generated_json}\")\n",
        "    if not os.path.exists(generated_image_jpg):\n",
        "        raise FileNotFoundError(f\"Skeleton image not found at {generated_image_jpg}\")\n",
        "\n",
        "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
        "    os.makedirs(os.path.dirname(output_image_path), exist_ok=True)\n",
        "\n",
        "    # Save JSON\n",
        "    os.replace(generated_json, output_json_path)\n",
        "    print(f\"📄 Saved keypoints JSON to: {output_json_path}\")\n",
        "\n",
        "    # Save JPG\n",
        "    os.replace(generated_image_jpg, output_image_path)\n",
        "    print(f\"🖼 Saved skeleton image to: {output_image_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0qh5G0snrcZ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import morphology\n",
        "from sklearn.cluster import KMeans\n",
        "import logging\n",
        "\n",
        "def create_clothing_mask(input_image_path, output_path=None, display_result=True,\n",
        "                        background_lower_bound=(0, 0, 220), background_upper_bound=(180, 20, 255),\n",
        "                        morph_kernel_size=3, morph_iterations=1, min_object_size=1000,\n",
        "                        use_grabcut=True):\n",
        "\n",
        "    logging.basicConfig(filename='clothing_mask_log.txt', level=logging.INFO,\n",
        "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "    try:\n",
        "        print(\"⚙ Initializing...\")\n",
        "\n",
        "        if not os.path.exists(input_image_path):\n",
        "            raise FileNotFoundError(f\"Input image not found: {input_image_path}\")\n",
        "\n",
        "        image = cv2.imread(input_image_path)\n",
        "        if image is None:\n",
        "            raise ValueError(f\"Invalid image file: {input_image_path}\")\n",
        "\n",
        "        print(f\"Processing image: {input_image_path}\")\n",
        "        logging.info(f\"Processing image: {input_image_path}\")\n",
        "\n",
        "        original_height, original_width = image.shape[:2]\n",
        "        logging.info(f\"Original dimensions: {original_width}x{original_height}\")\n",
        "\n",
        "        max_dim = 1024\n",
        "        if max(original_height, original_width) > max_dim:\n",
        "            scale = max_dim / max(original_height, original_width)\n",
        "            image = cv2.resize(image, (int(original_width * scale), int(original_height * scale)))\n",
        "            logging.info(f\"Resized to: {image.shape[1]}x{image.shape[0]}\")\n",
        "\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        inv_gamma = 1.0 / 0.8\n",
        "        table = np.array([((i / 255.0) ** inv_gamma) * 255 for i in np.arange(0, 256)]).astype(\"uint8\")\n",
        "        image_rgb = cv2.LUT(image_rgb, table)\n",
        "\n",
        "        lab = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2LAB)\n",
        "        l, a, b = cv2.split(lab)\n",
        "        clahe = cv2.createCLAHE(clipLimit=6.0, tileGridSize=(8, 8))\n",
        "        l = clahe.apply(l)\n",
        "        lab = cv2.merge((l, a, b))\n",
        "        image_rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
        "\n",
        "        image_rgb = cv2.bilateralFilter(image_rgb, d=9, sigmaColor=75, sigmaSpace=75)\n",
        "\n",
        "        # Additional RGB thresholding for blue channel\n",
        "        blue_channel = image_rgb[:, :, 2]\n",
        "        _, blue_mask = cv2.threshold(blue_channel, 150, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "        hsv = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2HSV)\n",
        "        pixel_values = image_rgb.reshape((-1, 3)).astype(np.float32)\n",
        "        kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "        labels = kmeans.fit_predict(pixel_values)\n",
        "        centers = kmeans.cluster_centers_.astype(np.uint8)\n",
        "        hsv_centers = cv2.cvtColor(centers.reshape(1, -1, 3), cv2.COLOR_RGB2HSV).reshape(-1, 3)\n",
        "        background_cluster = np.argmax(hsv_centers[:, 2])\n",
        "        binary = (labels.reshape(hsv.shape[:2]) != background_cluster).astype(np.uint8) * 255\n",
        "\n",
        "        # Combine with blue channel mask\n",
        "        binary = cv2.bitwise_or(binary, blue_mask)\n",
        "\n",
        "        if background_lower_bound is not None and background_upper_bound is not None:\n",
        "            color_mask = cv2.inRange(hsv, np.array(background_lower_bound), np.array(background_upper_bound))\n",
        "            binary = cv2.bitwise_and(binary, cv2.bitwise_not(color_mask))\n",
        "\n",
        "        # Save intermediate mask for debugging\n",
        "        logging.info(\"Saved intermediate mask after K-means and HSV\")\n",
        "\n",
        "        mask = binary // 255\n",
        "        scale_factor = min(image.shape[0], image.shape[1]) / 1024\n",
        "        adaptive_kernel_size = max(3, int(morph_kernel_size * scale_factor))\n",
        "        kernels = [\n",
        "            cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (adaptive_kernel_size, adaptive_kernel_size)),\n",
        "            cv2.getStructuringElement(cv2.MORPH_RECT, (adaptive_kernel_size, adaptive_kernel_size)),\n",
        "            cv2.getStructuringElement(cv2.MORPH_CROSS, (adaptive_kernel_size, adaptive_kernel_size))\n",
        "        ]\n",
        "\n",
        "        for i in range(morph_iterations):\n",
        "            kernel = kernels[i % len(kernels)]\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
        "            mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "        min_area = adaptive_kernel_size * adaptive_kernel_size * 4\n",
        "        mask = morphology.remove_small_objects(mask.astype(bool), min_size=min_area)\n",
        "        mask = morphology.remove_small_holes(mask, area_threshold=min_area)\n",
        "        mask = mask.astype(np.uint8) * 255\n",
        "\n",
        "        mask_float = mask.astype(np.float32) / 255.0\n",
        "        mask_float = cv2.bilateralFilter(mask_float, d=9, sigmaColor=0.1, sigmaSpace=5)\n",
        "        binary = (mask_float > 0.5).astype(np.uint8) * 255\n",
        "\n",
        "        # Save intermediate mask after morphological processing\n",
        "        logging.info(\"Saved intermediate mask after morphological processing\")\n",
        "\n",
        "\n",
        "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary, 8, cv2.CV_32S)\n",
        "        final_mask = np.zeros_like(binary)\n",
        "        adaptive_min_size = max(min_object_size, int(original_height * original_width * 0.005))\n",
        "        for i in range(1, num_labels):\n",
        "            if stats[i, cv2.CC_STAT_AREA] >= adaptive_min_size:\n",
        "                final_mask[labels == i] = 255\n",
        "\n",
        "        if max(original_height, original_width) > max_dim:\n",
        "            final_mask = cv2.resize(final_mask, (original_width, original_height),\n",
        "                                   interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        if output_path is None:\n",
        "            output_path = os.path.splitext(input_image_path)[0] + \"_mask.png\"\n",
        "        os.makedirs(os.path.dirname(output_path) or \".\", exist_ok=True)\n",
        "        cv2.imwrite(output_path, final_mask)\n",
        "        print(f\"✅ Saved mask to: {output_path}\")\n",
        "        logging.info(f\"Saved mask to: {output_path}\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {str(e)}\")\n",
        "        logging.error(f\"Error processing image {input_image_path}: {str(e)}\", exc_info=True)\n",
        "        raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAaC06CQyXfS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import cv2\n",
        "\n",
        "def human_parsing(input_image_path, output_path=None):\n",
        "    repo_dir = \"Self-Correction-Human-Parsing\"\n",
        "    input_filename = os.path.basename(input_image_path)\n",
        "    input_image_path_in_repo = f\"{repo_dir}/inputs/{input_filename}\"\n",
        "\n",
        "    img = cv2.imread(input_image_path)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Invalid image file: {input_image_path}\")\n",
        "    cv2.imwrite(input_image_path_in_repo, img)\n",
        "\n",
        "    # Run parsing\n",
        "    print(\"🔍 Parsing image...\")\n",
        "    cmd = (\n",
        "        f\"cd {repo_dir} && \"\n",
        "        f\"/usr/local/bin/python3.8 simple_extractor.py \"\n",
        "        f\"--dataset 'atr' \"\n",
        "        f\"--model-restore 'checkpoints/final.pth' \"\n",
        "        f\"--input-dir 'inputs' \"\n",
        "        f\"--output-dir 'outputs'\"\n",
        "    )\n",
        "\n",
        "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    print(\"----- STDOUT -----\\n\", result.stdout)\n",
        "    print(\"----- STDERR -----\\n\", result.stderr)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        raise RuntimeError(\"Parsing failed—see stderr above.\")\n",
        "\n",
        "    expected_output = f\"{repo_dir}/outputs/{input_filename.rsplit('.', 1)[0]}.png\"\n",
        "    if not os.path.exists(expected_output):\n",
        "        raise FileNotFoundError(\"Expected output not generated\")\n",
        "\n",
        "    if output_path:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        os.replace(expected_output, output_path)\n",
        "        return output_path\n",
        "\n",
        "    return expected_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vh2a6AcQpOnl",
        "outputId": "74c97271-e927-4860-9213-88d711d5d68d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ladi-vton'...\n",
            "remote: Enumerating objects: 109, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 109 (delta 25), reused 18 (delta 18), pack-reused 75 (from 1)\u001b[K\n",
            "Receiving objects: 100% (109/109), 1.54 MiB | 3.91 MiB/s, done.\n",
            "Resolving deltas: 100% (42/42), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/miccunifi/ladi-vton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO8EqPY8xrp8"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify, send_from_directory\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import requests\n",
        "import subprocess\n",
        "import os\n",
        "import firebase_admin\n",
        "from firebase_admin import credentials, firestore, auth\n",
        "import uuid\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "from IPython.display import display, HTML\n",
        "import shutil\n",
        "from flask_ngrok import run_with_ngrok\n",
        "\n",
        "# Initialize Firebase\n",
        "cred = credentials.Certificate(\"/content/drive/MyDrive/vton-yx1pe4-firebase-adminsdk-fbsvc-15844f65cd.json\")\n",
        "firebase_admin.initialize_app(cred)\n",
        "db = firestore.client()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpAU0ANlxtg6",
        "outputId": "b6b4538c-8edb-43ef-89f6-f9da926f36a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.1)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (3.0.2)\n",
            "Downloading flask_cors-6.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-6.0.1\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install flask-cors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "f_Rn9x6Cts6O",
        "outputId": "5a5ff0e9-158e-43d0-cb45-b46a2613a29e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "function keepAlive() {\n",
              "    console.log(\"Keeping Colab alive...\");\n",
              "    document.querySelector(\"colab-toolbar-button#connect\").click();\n",
              "}\n",
              "setInterval(keepAlive, 60 * 1000);\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from flask_cors import CORS\n",
        "from flask import Flask\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "run_with_ngrok(app)\n",
        "\n",
        "# Keep Colab alive\n",
        "display(HTML('''\n",
        "<script>\n",
        "function keepAlive() {\n",
        "    console.log(\"Keeping Colab alive...\");\n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click();\n",
        "}\n",
        "setInterval(keepAlive, 60 * 1000);\n",
        "</script>\n",
        "'''))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvh7XkFqSNlG"
      },
      "outputs": [],
      "source": [
        "# Python thread keep-alive\n",
        "import threading\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # Suppress TensorFlow warnings\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"  # Debugging CUDA errors\n",
        "os.environ[\"NVIDIA_TF32_OVERRIDE\"] = \"0\"  # Ensure numerical stability\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Ensure GPU is accessible\n",
        "\n",
        "def python_keep_alive():\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "        print(\"[Keep-Alive] Preventing timeout...\")\n",
        "\n",
        "keep_alive_thread = threading.Thread(target=python_keep_alive, daemon=True)\n",
        "keep_alive_thread.start()\n",
        "\n",
        "# Create directories\n",
        "os.makedirs('input_images', exist_ok=True)\n",
        "os.makedirs('output', exist_ok=True)\n",
        "os.makedirs('input_images/upper_body/keypoints', exist_ok=True)\n",
        "os.makedirs('input_images/upper_body/masks', exist_ok=True)\n",
        "os.makedirs('input_images/upper_body/label_maps', exist_ok=True)\n",
        "os.makedirs('input_images/upper_body/skeletons', exist_ok=True)\n",
        "os.makedirs('input_images/upper_body/images', exist_ok=True)\n",
        "\n",
        "os.makedirs('input_images/lower_body/keypoints', exist_ok=True)\n",
        "os.makedirs('input_images/lower_body/masks', exist_ok=True)\n",
        "os.makedirs('input_images/lower_body/label_maps', exist_ok=True)\n",
        "os.makedirs('input_images/lower_body/skeletons', exist_ok=True)\n",
        "os.makedirs('input_images/lower_body/images', exist_ok=True)\n",
        "\n",
        "os.makedirs('input_images/dresses/keypoints', exist_ok=True)\n",
        "os.makedirs('input_images/dresses/masks', exist_ok=True)\n",
        "os.makedirs('input_images/dresses/label_maps', exist_ok=True)\n",
        "os.makedirs('input_images/dresses/skeletons', exist_ok=True)\n",
        "os.makedirs('input_images/dresses/images', exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioWx66ZB9Wkh"
      },
      "outputs": [],
      "source": [
        "import cloudinary\n",
        "import cloudinary.uploader\n",
        "import os\n",
        "\n",
        "# Configure Cloudinary\n",
        "cloudinary.config(\n",
        "    cloud_name=\"dpl6zfv0y\",\n",
        "    api_key=\"551583568976988\",\n",
        "    api_secret=\"qWW_j0So54d7g5aW69GRUXHVC9g\"\n",
        ")\n",
        "\n",
        "def upload_and_get_public_url(image_path):\n",
        "    # Uploads an image to Cloudinary and returns its public URL\n",
        "\n",
        "    try:\n",
        "        response = cloudinary.uploader.upload(image_path)\n",
        "        cloudinary_url = response['secure_url']\n",
        "        print(f\"Image uploaded to Cloudinary. URL: {cloudinary_url}\")\n",
        "        return cloudinary_url\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAOBt4GBsImT"
      },
      "outputs": [],
      "source": [
        "# Function to create folder and save images, masks, and other data\n",
        "def add_folder(category_name, person_img, cloth_img):\n",
        "    if category_name == \"Upper\":\n",
        "        unique_id = \"1111\"\n",
        "        upper_body_folder = 'input_images/upper_body'\n",
        "        images_folder = os.path.join(upper_body_folder, \"images\")\n",
        "        os.makedirs(images_folder, exist_ok=True)\n",
        "        cat=\"upper_body\"\n",
        "\n",
        "        # Define filenames\n",
        "        person_filename = f\"{unique_id}_0.jpg\"\n",
        "        cloth_filename = f\"{unique_id}_1.jpg\"\n",
        "        output_filename = f\"{unique_id}_0.png\"  # Match person image name, different extension\n",
        "\n",
        "        # Full paths\n",
        "        person_path = os.path.join(images_folder, person_filename)\n",
        "        cloth_path = os.path.join(images_folder, cloth_filename)\n",
        "        output_path = f\"/content/output/unpaired/upper_body/{output_filename}\"\n",
        "\n",
        "        # Save images\n",
        "        person_img.save(person_path)\n",
        "        cloth_img.save(cloth_path)\n",
        "\n",
        "        output_labelmask_path = \"input_images/upper_body/label_maps/1111_4.png\"\n",
        "        result_path = human_parsing(person_path, output_labelmask_path)\n",
        "\n",
        "        output_clothmask_path = \"input_images/upper_body/masks/1111_1.png\"\n",
        "        create_clothing_mask(cloth_path, output_clothmask_path)\n",
        "\n",
        "        output_json_path = \"input_images/upper_body/keypoints/1111_2.json\"\n",
        "        output_skeletons_path = \"input_images/upper_body/skeletons/1111_5.png\"\n",
        "        extract_keypoints_skeletons(person_path, output_json_path, output_skeletons_path)\n",
        "\n",
        "        # Prepare test pairs file\n",
        "        test_pairs_path = f\"{upper_body_folder}/test_pairs_unpaired.txt\"\n",
        "        with open(test_pairs_path, 'w') as f:\n",
        "            f.write(f\"{os.path.basename(person_path)}    {os.path.basename(cloth_path)}\\n\")\n",
        "        shutil.copy(test_pairs_path, \"input_images/test_pairs_paired.txt\")\n",
        "        test_path = '/content/input_images/test_pairs_paired.txt'\n",
        "        return cat,test_pairs_path, person_path, cloth_path, test_path, output_labelmask_path, output_clothmask_path, output_json_path, output_skeletons_path, output_filename\n",
        "\n",
        "    elif category_name == \"Lower\":\n",
        "        unique_id = \"1111\"\n",
        "        lower_body_folder = 'input_images/lower_body'\n",
        "        images_folder = os.path.join(lower_body_folder, \"images\")\n",
        "        os.makedirs(images_folder, exist_ok=True)\n",
        "        cat=\"lower_body\"\n",
        "\n",
        "        # Define filenames\n",
        "        person_filename = f\"{unique_id}_0.jpg\"\n",
        "        cloth_filename = f\"{unique_id}_1.jpg\"\n",
        "        output_filename = f\"{unique_id}_0.png\"  # Match person image name, different extension\n",
        "\n",
        "        # Full paths\n",
        "        person_path = os.path.join(images_folder, person_filename)\n",
        "        cloth_path = os.path.join(images_folder, cloth_filename)\n",
        "        output_path = f\"/content/output/unpaired/lower_body/{output_filename}\"\n",
        "\n",
        "        # Save images\n",
        "        person_img.save(person_path)\n",
        "        cloth_img.save(cloth_path)\n",
        "\n",
        "        output_labelmask_path = \"input_images/lower_body/label_maps/1111_4.png\"\n",
        "        result_path = human_parsing(person_path, output_labelmask_path)\n",
        "\n",
        "        output_clothmask_path = \"input_images/lower_body/masks/1111_1.png\"\n",
        "        create_clothing_mask(cloth_path, output_clothmask_path)\n",
        "\n",
        "        output_json_path = \"input_images/lower_body/keypoints/1111_2.json\"\n",
        "        output_skeletons_path = \"input_images/lower_body/skeletons/1111_5.png\"\n",
        "        extract_keypoints_skeletons(person_path, output_json_path, output_skeletons_path)\n",
        "\n",
        "        # Prepare test pairs file\n",
        "        test_pairs_path = f\"{lower_body_folder}/test_pairs_unpaired.txt\"\n",
        "        with open(test_pairs_path, 'w') as f:\n",
        "            f.write(f\"{os.path.basename(person_path)}    {os.path.basename(cloth_path)}\\n\")\n",
        "        shutil.copy(test_pairs_path, \"input_images/test_pairs_paired.txt\")\n",
        "        test_path = '/content/input_images/test_pairs_paired.txt'\n",
        "        return cat,test_pairs_path, person_path, cloth_path, test_path, output_labelmask_path, output_clothmask_path, output_json_path, output_skeletons_path, output_filename\n",
        "\n",
        "    elif category_name == \"Full\":\n",
        "        unique_id = \"1111\"\n",
        "        dresses_folder = 'input_images/dresses'\n",
        "        images_folder = os.path.join(dresses_folder, \"images\")\n",
        "        os.makedirs(images_folder, exist_ok=True)\n",
        "        cat=\"dresses\"\n",
        "\n",
        "        # Define filenames\n",
        "        person_filename = f\"{unique_id}_0.jpg\"\n",
        "        cloth_filename = f\"{unique_id}_1.jpg\"\n",
        "        output_filename = f\"{unique_id}_0.png\"  # Match person image name, different extension\n",
        "\n",
        "        # Full paths\n",
        "        person_path = os.path.join(images_folder, person_filename)\n",
        "        cloth_path = os.path.join(images_folder, cloth_filename)\n",
        "        output_path = f\"/content/output/unpaired/dresses/{output_filename}\"\n",
        "\n",
        "        # Save images\n",
        "        person_img.save(person_path)\n",
        "        cloth_img.save(cloth_path)\n",
        "\n",
        "        output_labelmask_path = \"input_images/dresses/label_maps/1111_4.png\"\n",
        "        result_path = human_parsing(person_path, output_labelmask_path)\n",
        "\n",
        "        output_clothmask_path = \"input_images/dresses/masks/1111_1.png\"\n",
        "        create_clothing_mask(cloth_path, output_clothmask_path)\n",
        "\n",
        "        output_json_path = \"input_images/dresses/keypoints/1111_2.json\"\n",
        "        output_skeletons_path = \"input_images/dresses/skeletons/1111_5.png\"\n",
        "        extract_keypoints_skeletons(person_path, output_json_path, output_skeletons_path)\n",
        "\n",
        "        # Prepare test pairs file\n",
        "        test_pairs_path = f\"{dresses_folder}/test_pairs_unpaired.txt\"\n",
        "        with open(test_pairs_path, 'w') as f:\n",
        "            f.write(f\"{os.path.basename(person_path)}    {os.path.basename(cloth_path)}\\n\")\n",
        "        shutil.copy(test_pairs_path, \"input_images/test_pairs_paired.txt\")\n",
        "        test_path = '/content/input_images/test_pairs_paired.txt'\n",
        "        return cat,test_pairs_path, person_path, cloth_path, test_path, output_labelmask_path, output_clothmask_path, output_json_path, output_skeletons_path, output_filename\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the Python file you want to delete\n",
        "file_path = '/content/ladi-vton/src/inference.py'\n",
        "\n",
        "# Delete the file\n",
        "if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "    print(f\"{file_path} has been deleted.\")\n",
        "else:\n",
        "    print(f\"{file_path} does not exist.\")\n",
        "\n",
        "!cp -r /content/drive/MyDrive/inference.py  /content/ladi-vton/src\n",
        "print(\"donnne\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJotAJQdAD1_",
        "outputId": "58ed87b1-a07c-4211-9ebd-05f542c7e64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ladi-vton/src/inference.py has been deleted.\n",
            "donnne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Path to the Python file you want to delete\n",
        "file_path = '/content/ladi-vton/src/models/ConvNet_TPS.py'\n",
        "\n",
        "# Delete the file\n",
        "if os.path.exists(file_path):\n",
        "    os.remove(file_path)\n",
        "    print(f\"{file_path} has been deleted.\")\n",
        "else:\n",
        "    print(f\"{file_path} does not exist.\")\n",
        "\n",
        "!cp -r /content/drive/MyDrive/ConvNet_TPS.py  /content/ladi-vton/src/models\n",
        "print(\"donnne\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06af0b1d-5d52-4f80-9dfe-d0cb748c148c",
        "id": "v0G3ZLS8tTVu"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ladi-vton/src/ConvNet_TPS.py does not exist.\n",
            "donnne\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raMzqsYpNEf2"
      },
      "outputs": [],
      "source": [
        "@app.route('/run_all', methods=['POST'])\n",
        "def run_all():\n",
        "    print(f\"\\n=== New Request Received ===\")\n",
        "    try:\n",
        "        # checking the content type\n",
        "        if not request.is_json:\n",
        "            return jsonify({\"error\": \"Content-Type must be application/json\"}), 415\n",
        "\n",
        "        # get the data and check it\n",
        "        data = request.get_json()\n",
        "        user_id = data.get('user_id')\n",
        "        if not user_id:\n",
        "            return jsonify({\"error\": \"user_id is required\"}), 400\n",
        "\n",
        "        # find a new request that didn't treat yet\n",
        "        query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n",
        "        unprocessed_doc = None\n",
        "\n",
        "        for doc in query:\n",
        "            user_data = doc.to_dict()\n",
        "            if user_data.get(\"status\", \"\") != \"done\":\n",
        "                unprocessed_doc = doc\n",
        "                break\n",
        "\n",
        "        if not unprocessed_doc:\n",
        "            return jsonify({\"error\": \"No pending try-on requests for this user\"}), 404\n",
        "\n",
        "        doc = unprocessed_doc\n",
        "        user_data = doc.to_dict()\n",
        "        doc_ref = db.collection(\"TryOn\").document(doc.id)\n",
        "\n",
        "        # check the doc before update\n",
        "        if not doc_ref.get().exists:\n",
        "            print(f\"❌ Document {doc.id} does not exist. Cannot update.\")\n",
        "            return jsonify({\"error\": \"Document not found for update\"}), 404\n",
        "\n",
        "        doc_ref.update({\"status\": \"processing\"})  # update to \"processing\"\n",
        "        print(f\"✅ Updated request status to 'processing'.\")\n",
        "\n",
        "        person_url = user_data.get('model_img')\n",
        "        cloth_url = user_data.get('cloth_img')\n",
        "\n",
        "        if not person_url or not cloth_url:\n",
        "            return jsonify({\"error\": \"Missing image URLs\"}), 400\n",
        "\n",
        "        # download the image\n",
        "        try:\n",
        "            person_response = requests.get(person_url, timeout=10)\n",
        "            person_response.raise_for_status()\n",
        "            cloth_response = requests.get(cloth_url, timeout=10)\n",
        "            cloth_response.raise_for_status()\n",
        "\n",
        "            person_img = Image.open(BytesIO(person_response.content))\n",
        "            cloth_img = Image.open(BytesIO(cloth_response.content))\n",
        "            print(f\"✅ Successfully downloaded images.\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            return jsonify({\"error\": f\"Image download failed: {str(e)}\"}), 500\n",
        "\n",
        "        category_name = check_category(cloth_url)\n",
        "        cat, test_pairs_path, person_path, cloth_path, test_path, output_labelmask_path, output_clothmask_path, output_json_path, output_skeletons_path, output_filename = add_folder(category_name, person_img, cloth_img)\n",
        "        print(\"file_Nammmmmmme\", output_filename)\n",
        "        output_path = f\"/content/output/unpaired/{cat}/{output_filename}\"\n",
        "        print(\"file_Nammmmmmme_paaaathhhhh\", output_path)\n",
        "        print(\"Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme\", category_name)\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "        def run_inference():\n",
        "            print(\"🔁 Background: Starting inference...\")\n",
        "            try:\n",
        "\n",
        "                result = subprocess.run([\n",
        "                  \"python\", \"/content/ladi-vton/src/inference.py\",\n",
        "                    \"--dataset\", \"dresscode\",\n",
        "                    \"--dresscode_dataroot\", \"/content/input_images\",\n",
        "                    \"--output_dir\", \"/content/output\",\n",
        "                    \"--test_order\", \"unpaired\",\n",
        "                    \"--category\", f\"{cat}\",\n",
        "                    \"--enable_xformers_memory_efficient_attention\",\n",
        "                    \"--use_png\",\n",
        "                    \"--compute_metrics\",\n",
        "                    \"--batch_size\", \"1\",\n",
        "                    \"--num_workers\", \"0\",\n",
        "                    \"--mixed_precision\", \"no\",\n",
        "\n",
        "              ], capture_output=True, text=True)\n",
        "\n",
        "                print(f\"✅ Inference stdout:\\n{result.stdout}\")\n",
        "                if result.stderr:\n",
        "                    print(f\"⚠️ Inference stderr (non-fatal):\\n{result.stderr}\")\n",
        "\n",
        "                if os.path.exists(output_path):\n",
        "                    final_url = upload_and_get_public_url(output_path)\n",
        "                    if final_url:\n",
        "                        print(f\"✅ Result URL: {final_url}\")\n",
        "                        doc_ref.update({\"output_img\": final_url, \"status\": \"done\", \"category_name\": category_name})\n",
        "                        print(\"✅ Inference done and result saved.\")\n",
        "                    else:\n",
        "                        doc_ref.update({\"output_img\": \"error\", \"status\": \"upload_failed\"})\n",
        "                        print(\"❌ Failed to upload output image.\")\n",
        "                else:\n",
        "                    doc_ref.update({\"output_img\": \"error\", \"status\": \"no_output\"})\n",
        "                    print(\"❌ No output image generated.\")\n",
        "\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                print(f\"❌ Inference failed: {e.stderr}\")\n",
        "                doc_ref.update({\"output_img\": \"error\", \"status\": \"inference_failed\"})\n",
        "\n",
        "            finally:\n",
        "                try:\n",
        "                    for file_path in [test_pairs_path, person_path, cloth_path, test_path, output_labelmask_path, output_clothmask_path, output_json_path, output_skeletons_path]:\n",
        "                        if os.path.exists(file_path):\n",
        "                            os.remove(file_path)\n",
        "                    print(\"🧹 Cleaned up temp files.\")\n",
        "                except Exception as cleanup_error:\n",
        "                    print(f\"⚠️ Cleanup error: {cleanup_error}\")\n",
        "\n",
        "        threading.Thread(target=run_inference, daemon=True).start()\n",
        "\n",
        "        return jsonify({\n",
        "            \"success\": True,\n",
        "            \"message\": \"Processing started\",\n",
        "            \"user_id\": user_id\n",
        "        }), 202\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Server error: {str(e)}\")\n",
        "        return jsonify({\"error\": \"Internal server error\", \"details\": str(e)}), 500\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gd3QeIAHxqjA",
        "outputId": "c309f3ed-ecdc-400e-c035-625f0454b9f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "Public URL: NgrokTunnel: \"https://ultimately-stable-lark.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        }
      ],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from pyngrok import ngrok\n",
        "\n",
        "!ngrok config add-authtoken 2wKBK5nBSIfiMLhExX5HsOqqfbO_5KHZQY7ZXz2EbAyYDF4td\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "# use a specific domain\n",
        "public_url = ngrok.connect(addr=5000, domain=\"ultimately-stable-lark.ngrok-free.app\")\n",
        "print(\"Public URL:\", public_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7YVvfLFxs3w",
        "outputId": "60fd69ac-1f46-41b9-f6f7-3db78b37ad93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Running on http://ultimately-stable-lark.ngrok-free.app\n",
            " * Traffic stats available on http://127.0.0.1:4040\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/lower_body/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/lower_body/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 16:04:05] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/lower_body/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/lower_body/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/lower_body/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Lower\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "Found 1 images in the folder /tmp/dresscode/lower_body\n",
            "saving custom FID stats to /usr/local/lib/python3.11/dist-packages/cleanfid/stats/dresscode_lower_body_clean_custom_na.npz\n",
            "saving custom KID stats to /usr/local/lib/python3.11/dist-packages/cleanfid/stats/dresscode_lower_body_clean_custom_na_kid.npz\n",
            "Found 0 images in the folder /tmp/dresscode/upper_body\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751126649.716643   27731 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751126649.722849   27731 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "Downloading scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]\n",
            "Downloading scheduler_config.json: 100%|██████████| 308/308 [00:00<00:00, 1.99MB/s]\n",
            "\n",
            "Downloading config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]\n",
            "Downloading config.json: 100%|██████████| 638/638 [00:00<00:00, 4.09MB/s]\n",
            "\n",
            "Downloading model.safetensors:   0%|          | 0.00/1.36G [00:00<?, ?B/s]\n",
            "Downloading model.safetensors:   2%|▏         | 21.0M/1.36G [00:00<00:09, 144MB/s]\n",
            "Downloading model.safetensors:   4%|▍         | 52.4M/1.36G [00:00<00:05, 221MB/s]\n",
            "Downloading model.safetensors:   6%|▌         | 83.9M/1.36G [00:00<00:05, 245MB/s]\n",
            "Downloading model.safetensors:   8%|▊         | 115M/1.36G [00:00<00:04, 252MB/s] \n",
            "Downloading model.safetensors:  11%|█         | 147M/1.36G [00:00<00:05, 231MB/s]\n",
            "Downloading model.safetensors:  13%|█▎        | 178M/1.36G [00:00<00:05, 236MB/s]\n",
            "Downloading model.safetensors:  15%|█▌        | 210M/1.36G [00:00<00:04, 242MB/s]\n",
            "Downloading model.safetensors:  18%|█▊        | 241M/1.36G [00:01<00:04, 247MB/s]\n",
            "Downloading model.safetensors:  20%|██        | 273M/1.36G [00:01<00:04, 243MB/s]\n",
            "Downloading model.safetensors:  22%|██▏       | 304M/1.36G [00:01<00:04, 254MB/s]\n",
            "Downloading model.safetensors:  25%|██▍       | 336M/1.36G [00:01<00:04, 249MB/s]\n",
            "Downloading model.safetensors:  27%|██▋       | 367M/1.36G [00:01<00:03, 256MB/s]\n",
            "Downloading model.safetensors:  29%|██▉       | 398M/1.36G [00:01<00:03, 252MB/s]\n",
            "Downloading model.safetensors:  32%|███▏      | 430M/1.36G [00:01<00:03, 249MB/s]\n",
            "Downloading model.safetensors:  34%|███▍      | 461M/1.36G [00:01<00:03, 249MB/s]\n",
            "Downloading model.safetensors:  36%|███▌      | 493M/1.36G [00:02<00:03, 259MB/s]\n",
            "Downloading model.safetensors:  39%|███▊      | 524M/1.36G [00:02<00:03, 258MB/s]\n",
            "Downloading model.safetensors:  41%|████      | 556M/1.36G [00:02<00:03, 256MB/s]\n",
            "Downloading model.safetensors:  43%|████▎     | 587M/1.36G [00:02<00:03, 258MB/s]\n",
            "Downloading model.safetensors:  45%|████▌     | 619M/1.36G [00:02<00:02, 251MB/s]\n",
            "Downloading model.safetensors:  48%|████▊     | 650M/1.36G [00:02<00:02, 252MB/s]\n",
            "Downloading model.safetensors:  50%|█████     | 682M/1.36G [00:02<00:02, 249MB/s]\n",
            "Downloading model.safetensors:  52%|█████▏    | 713M/1.36G [00:02<00:02, 253MB/s]\n",
            "Downloading model.safetensors:  55%|█████▍    | 744M/1.36G [00:02<00:02, 258MB/s]\n",
            "Downloading model.safetensors:  57%|█████▋    | 776M/1.36G [00:03<00:02, 256MB/s]\n",
            "Downloading model.safetensors:  59%|█████▉    | 807M/1.36G [00:03<00:02, 253MB/s]\n",
            "Downloading model.safetensors:  62%|██████▏   | 839M/1.36G [00:03<00:02, 257MB/s]\n",
            "Downloading model.safetensors:  64%|██████▍   | 870M/1.36G [00:03<00:01, 258MB/s]\n",
            "Downloading model.safetensors:  66%|██████▌   | 902M/1.36G [00:03<00:01, 253MB/s]\n",
            "Downloading model.safetensors:  69%|██████▊   | 933M/1.36G [00:03<00:01, 252MB/s]\n",
            "Downloading model.safetensors:  71%|███████   | 965M/1.36G [00:03<00:01, 252MB/s]\n",
            "Downloading model.safetensors:  73%|███████▎  | 996M/1.36G [00:03<00:01, 251MB/s]\n",
            "Downloading model.safetensors:  75%|███████▌  | 1.03G/1.36G [00:04<00:01, 255MB/s]\n",
            "Downloading model.safetensors:  78%|███████▊  | 1.06G/1.36G [00:04<00:01, 254MB/s]\n",
            "Downloading model.safetensors:  80%|████████  | 1.09G/1.36G [00:04<00:01, 255MB/s]\n",
            "Downloading model.safetensors:  82%|████████▏ | 1.12G/1.36G [00:04<00:00, 259MB/s]\n",
            "Downloading model.safetensors:  85%|████████▍ | 1.15G/1.36G [00:04<00:00, 259MB/s]\n",
            "Downloading model.safetensors:  87%|████████▋ | 1.18G/1.36G [00:04<00:00, 261MB/s]\n",
            "Downloading model.safetensors:  89%|████████▉ | 1.22G/1.36G [00:04<00:00, 252MB/s]\n",
            "Downloading model.safetensors:  92%|█████████▏| 1.25G/1.36G [00:04<00:00, 250MB/s]\n",
            "Downloading model.safetensors:  94%|█████████▍| 1.28G/1.36G [00:05<00:00, 251MB/s]\n",
            "Downloading model.safetensors:  96%|█████████▋| 1.31G/1.36G [00:05<00:00, 260MB/s]\n",
            "Downloading model.safetensors:  99%|█████████▊| 1.34G/1.36G [00:05<00:00, 253MB/s]\n",
            "Downloading model.safetensors: 100%|██████████| 1.36G/1.36G [00:05<00:00, 250MB/s]\n",
            "\n",
            "Downloading (…)ch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]\n",
            "Downloading (…)ch_model.safetensors:   9%|▉         | 31.5M/335M [00:00<00:01, 275MB/s]\n",
            "Downloading (…)ch_model.safetensors:  19%|█▉        | 62.9M/335M [00:00<00:00, 274MB/s]\n",
            "Downloading (…)ch_model.safetensors:  28%|██▊       | 94.4M/335M [00:00<00:01, 149MB/s]\n",
            "Downloading (…)ch_model.safetensors:  38%|███▊      | 126M/335M [00:00<00:01, 173MB/s] \n",
            "Downloading (…)ch_model.safetensors:  47%|████▋     | 157M/335M [00:00<00:00, 192MB/s]\n",
            "Downloading (…)ch_model.safetensors:  56%|█████▋    | 189M/335M [00:00<00:00, 210MB/s]\n",
            "Downloading (…)ch_model.safetensors:  66%|██████▌   | 220M/335M [00:01<00:00, 225MB/s]\n",
            "Downloading (…)ch_model.safetensors:  75%|███████▌  | 252M/335M [00:01<00:00, 236MB/s]\n",
            "Downloading (…)ch_model.safetensors:  85%|████████▍ | 283M/335M [00:01<00:00, 237MB/s]\n",
            "Downloading (…)ch_model.safetensors:  94%|█████████▍| 315M/335M [00:01<00:00, 241MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100%|██████████| 335M/335M [00:01<00:00, 219MB/s]\n",
            "\n",
            "Downloading config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]\n",
            "Downloading config.json: 100%|██████████| 616/616 [00:00<00:00, 4.42MB/s]\n",
            "\n",
            "Downloading config.json: 0.00B [00:00, ?B/s]\n",
            "Downloading config.json: 4.72kB [00:00, 18.7MB/s]\n",
            "\n",
            "Downloading model.safetensors:   0%|          | 0.00/3.94G [00:00<?, ?B/s]\n",
            "Downloading model.safetensors:   1%|          | 41.9M/3.94G [00:00<00:12, 324MB/s]\n",
            "Downloading model.safetensors:   2%|▏         | 83.9M/3.94G [00:00<00:13, 292MB/s]\n",
            "Downloading model.safetensors:   3%|▎         | 115M/3.94G [00:00<00:14, 259MB/s] \n",
            "Downloading model.safetensors:   4%|▎         | 147M/3.94G [00:00<00:14, 258MB/s]\n",
            "Downloading model.safetensors:   5%|▍         | 178M/3.94G [00:00<00:14, 262MB/s]\n",
            "Downloading model.safetensors:   5%|▌         | 210M/3.94G [00:00<00:14, 263MB/s]\n",
            "Downloading model.safetensors:   6%|▌         | 241M/3.94G [00:03<01:58, 31.2MB/s]\n",
            "Downloading model.safetensors:   7%|▋         | 273M/3.94G [00:03<01:25, 43.1MB/s]\n",
            "Downloading model.safetensors:   8%|▊         | 304M/3.94G [00:03<01:02, 58.3MB/s]\n",
            "Downloading model.safetensors:   9%|▊         | 336M/3.94G [00:04<00:47, 76.5MB/s]\n",
            "Downloading model.safetensors:   9%|▉         | 367M/3.94G [00:04<00:37, 96.2MB/s]\n",
            "Downloading model.safetensors:  10%|█         | 398M/3.94G [00:04<00:29, 119MB/s] \n",
            "Downloading model.safetensors:  11%|█         | 430M/3.94G [00:04<00:24, 143MB/s]\n",
            "Downloading model.safetensors:  12%|█▏        | 461M/3.94G [00:04<00:21, 162MB/s]\n",
            "Downloading model.safetensors:  12%|█▏        | 493M/3.94G [00:04<00:18, 183MB/s]\n",
            "Downloading model.safetensors:  13%|█▎        | 524M/3.94G [00:04<00:16, 203MB/s]\n",
            "Downloading model.safetensors:  14%|█▍        | 556M/3.94G [00:04<00:15, 220MB/s]\n",
            "Downloading model.safetensors:  15%|█▍        | 587M/3.94G [00:05<00:14, 231MB/s]\n",
            "Downloading model.safetensors:  16%|█▌        | 619M/3.94G [00:05<00:13, 241MB/s]\n",
            "Downloading model.safetensors:  16%|█▋        | 650M/3.94G [00:05<00:13, 248MB/s]\n",
            "Downloading model.safetensors:  17%|█▋        | 682M/3.94G [00:05<00:12, 252MB/s]\n",
            "Downloading model.safetensors:  18%|█▊        | 713M/3.94G [00:05<00:12, 254MB/s]\n",
            "Downloading model.safetensors:  19%|█▉        | 744M/3.94G [00:05<00:12, 255MB/s]\n",
            "Downloading model.safetensors:  20%|█▉        | 776M/3.94G [00:05<00:12, 251MB/s]\n",
            "Downloading model.safetensors:  20%|██        | 807M/3.94G [00:05<00:12, 258MB/s]\n",
            "Downloading model.safetensors:  21%|██▏       | 839M/3.94G [00:05<00:11, 262MB/s]\n",
            "Downloading model.safetensors:  22%|██▏       | 870M/3.94G [00:06<00:11, 263MB/s]\n",
            "Downloading model.safetensors:  23%|██▎       | 902M/3.94G [00:06<00:11, 263MB/s]\n",
            "Downloading model.safetensors:  24%|██▎       | 933M/3.94G [00:06<00:11, 270MB/s]\n",
            "Downloading model.safetensors:  24%|██▍       | 965M/3.94G [00:06<00:10, 273MB/s]\n",
            "Downloading model.safetensors:  25%|██▌       | 996M/3.94G [00:06<00:10, 273MB/s]\n",
            "Downloading model.safetensors:  26%|██▌       | 1.03G/3.94G [00:06<00:10, 266MB/s]\n",
            "Downloading model.safetensors:  27%|██▋       | 1.06G/3.94G [00:06<00:10, 267MB/s]\n",
            "Downloading model.safetensors:  28%|██▊       | 1.09G/3.94G [00:06<00:10, 266MB/s]\n",
            "Downloading model.safetensors:  28%|██▊       | 1.12G/3.94G [00:07<00:10, 267MB/s]\n",
            "Downloading model.safetensors:  29%|██▉       | 1.15G/3.94G [00:07<00:10, 266MB/s]\n",
            "Downloading model.safetensors:  30%|███       | 1.18G/3.94G [00:07<00:10, 262MB/s]\n",
            "Downloading model.safetensors:  31%|███       | 1.22G/3.94G [00:07<00:10, 259MB/s]\n",
            "Downloading model.safetensors:  32%|███▏      | 1.25G/3.94G [00:07<00:10, 268MB/s]\n",
            "Downloading model.safetensors:  32%|███▏      | 1.28G/3.94G [00:07<00:10, 252MB/s]\n",
            "Downloading model.safetensors:  33%|███▎      | 1.31G/3.94G [00:07<00:10, 251MB/s]\n",
            "Downloading model.safetensors:  34%|███▍      | 1.34G/3.94G [00:07<00:10, 256MB/s]\n",
            "Downloading model.safetensors:  35%|███▍      | 1.37G/3.94G [00:08<00:09, 262MB/s]\n",
            "Downloading model.safetensors:  36%|███▌      | 1.41G/3.94G [00:08<00:09, 261MB/s]\n",
            "Downloading model.safetensors:  36%|███▋      | 1.44G/3.94G [00:08<00:09, 263MB/s]\n",
            "Downloading model.safetensors:  37%|███▋      | 1.47G/3.94G [00:08<00:09, 258MB/s]\n",
            "Downloading model.safetensors:  38%|███▊      | 1.50G/3.94G [00:08<00:09, 263MB/s]\n",
            "Downloading model.safetensors:  39%|███▉      | 1.53G/3.94G [00:08<00:09, 261MB/s]\n",
            "Downloading model.safetensors:  40%|███▉      | 1.56G/3.94G [00:08<00:09, 256MB/s]\n",
            "Downloading model.safetensors:  40%|████      | 1.59G/3.94G [00:08<00:09, 253MB/s]\n",
            "Downloading model.safetensors:  41%|████      | 1.63G/3.94G [00:09<00:09, 238MB/s]\n",
            "Downloading model.safetensors:  42%|████▏     | 1.66G/3.94G [00:09<00:09, 233MB/s]\n",
            "Downloading model.safetensors:  43%|████▎     | 1.69G/3.94G [00:09<00:09, 229MB/s]\n",
            "Downloading model.safetensors:  44%|████▎     | 1.72G/3.94G [00:09<00:09, 234MB/s]\n",
            "Downloading model.safetensors:  44%|████▍     | 1.75G/3.94G [00:09<00:09, 224MB/s]\n",
            "Downloading model.safetensors:  45%|████▌     | 1.78G/3.94G [00:09<00:09, 221MB/s]\n",
            "Downloading model.safetensors:  46%|████▌     | 1.81G/3.94G [00:09<00:09, 218MB/s]\n",
            "Downloading model.safetensors:  47%|████▋     | 1.85G/3.94G [00:10<00:09, 212MB/s]\n",
            "Downloading model.safetensors:  48%|████▊     | 1.88G/3.94G [00:10<00:09, 217MB/s]\n",
            "Downloading model.safetensors:  48%|████▊     | 1.91G/3.94G [00:10<00:09, 215MB/s]\n",
            "Downloading model.safetensors:  49%|████▉     | 1.94G/3.94G [00:10<00:08, 225MB/s]\n",
            "Downloading model.safetensors:  50%|████▉     | 1.97G/3.94G [00:10<00:09, 219MB/s]\n",
            "Downloading model.safetensors:  51%|█████     | 2.00G/3.94G [00:10<00:08, 229MB/s]\n",
            "Downloading model.safetensors:  52%|█████▏    | 2.03G/3.94G [00:10<00:08, 227MB/s]\n",
            "Downloading model.safetensors:  52%|█████▏    | 2.07G/3.94G [00:11<00:08, 232MB/s]\n",
            "Downloading model.safetensors:  53%|█████▎    | 2.10G/3.94G [00:11<00:08, 207MB/s]\n",
            "Downloading model.safetensors:  54%|█████▍    | 2.13G/3.94G [00:11<00:08, 213MB/s]\n",
            "Downloading model.safetensors:  55%|█████▍    | 2.16G/3.94G [00:11<00:08, 215MB/s]\n",
            "Downloading model.safetensors:  56%|█████▌    | 2.19G/3.94G [00:11<00:07, 222MB/s]\n",
            "Downloading model.safetensors:  56%|█████▋    | 2.22G/3.94G [00:11<00:07, 230MB/s]\n",
            "Downloading model.safetensors:  57%|█████▋    | 2.25G/3.94G [00:11<00:07, 234MB/s]\n",
            "Downloading model.safetensors:  58%|█████▊    | 2.29G/3.94G [00:12<00:07, 226MB/s]\n",
            "Downloading model.safetensors:  59%|█████▊    | 2.32G/3.94G [00:12<00:07, 227MB/s]\n",
            "Downloading model.safetensors:  60%|█████▉    | 2.35G/3.94G [00:12<00:06, 230MB/s]\n",
            "Downloading model.safetensors:  60%|██████    | 2.38G/3.94G [00:12<00:07, 221MB/s]\n",
            "Downloading model.safetensors:  61%|██████    | 2.41G/3.94G [00:12<00:06, 224MB/s]\n",
            "Downloading model.safetensors:  62%|██████▏   | 2.44G/3.94G [00:12<00:06, 228MB/s]\n",
            "Downloading model.safetensors:  63%|██████▎   | 2.47G/3.94G [00:13<00:15, 97.4MB/s]\n",
            "Downloading model.safetensors:  64%|██████▎   | 2.51G/3.94G [00:13<00:11, 121MB/s] \n",
            "Downloading model.safetensors:  64%|██████▍   | 2.54G/3.94G [00:13<00:09, 143MB/s]\n",
            "Downloading model.safetensors:  65%|██████▌   | 2.57G/3.94G [00:13<00:08, 163MB/s]\n",
            "Downloading model.safetensors:  66%|██████▌   | 2.60G/3.94G [00:13<00:07, 182MB/s]\n",
            "Downloading model.safetensors:  67%|██████▋   | 2.63G/3.94G [00:14<00:06, 194MB/s]\n",
            "Downloading model.safetensors:  68%|██████▊   | 2.66G/3.94G [00:14<00:06, 203MB/s]\n",
            "Downloading model.safetensors:  68%|██████▊   | 2.69G/3.94G [00:14<00:05, 217MB/s]\n",
            "Downloading model.safetensors:  69%|██████▉   | 2.73G/3.94G [00:14<00:05, 230MB/s]\n",
            "Downloading model.safetensors:  70%|██████▉   | 2.76G/3.94G [00:14<00:04, 243MB/s]\n",
            "Downloading model.safetensors:  71%|███████   | 2.79G/3.94G [00:14<00:04, 244MB/s]\n",
            "Downloading model.safetensors:  72%|███████▏  | 2.82G/3.94G [00:14<00:04, 247MB/s]\n",
            "Downloading model.safetensors:  72%|███████▏  | 2.85G/3.94G [00:14<00:04, 250MB/s]\n",
            "Downloading model.safetensors:  73%|███████▎  | 2.88G/3.94G [00:15<00:04, 254MB/s]\n",
            "Downloading model.safetensors:  74%|███████▍  | 2.92G/3.94G [00:15<00:04, 254MB/s]\n",
            "Downloading model.safetensors:  75%|███████▍  | 2.95G/3.94G [00:15<00:03, 258MB/s]\n",
            "Downloading model.safetensors:  75%|███████▌  | 2.98G/3.94G [00:15<00:03, 265MB/s]\n",
            "Downloading model.safetensors:  76%|███████▋  | 3.01G/3.94G [00:15<00:03, 240MB/s]\n",
            "Downloading model.safetensors:  77%|███████▋  | 3.04G/3.94G [00:15<00:03, 248MB/s]\n",
            "Downloading model.safetensors:  78%|███████▊  | 3.07G/3.94G [00:15<00:03, 247MB/s]\n",
            "Downloading model.safetensors:  79%|███████▊  | 3.10G/3.94G [00:15<00:03, 254MB/s]\n",
            "Downloading model.safetensors:  79%|███████▉  | 3.14G/3.94G [00:16<00:03, 255MB/s]\n",
            "Downloading model.safetensors:  80%|████████  | 3.17G/3.94G [00:16<00:02, 260MB/s]\n",
            "Downloading model.safetensors:  81%|████████  | 3.20G/3.94G [00:16<00:04, 169MB/s]\n",
            "Downloading model.safetensors:  82%|████████▏ | 3.23G/3.94G [00:16<00:03, 193MB/s]\n",
            "Downloading model.safetensors:  83%|████████▎ | 3.26G/3.94G [00:16<00:03, 207MB/s]\n",
            "Downloading model.safetensors:  83%|████████▎ | 3.29G/3.94G [00:16<00:02, 219MB/s]\n",
            "Downloading model.safetensors:  84%|████████▍ | 3.32G/3.94G [00:16<00:02, 233MB/s]\n",
            "Downloading model.safetensors:  85%|████████▌ | 3.36G/3.94G [00:17<00:02, 239MB/s]\n",
            "Downloading model.safetensors:  86%|████████▌ | 3.39G/3.94G [00:17<00:02, 243MB/s]\n",
            "Downloading model.safetensors:  87%|████████▋ | 3.42G/3.94G [00:17<00:02, 250MB/s]\n",
            "Downloading model.safetensors:  87%|████████▋ | 3.45G/3.94G [00:17<00:01, 254MB/s]\n",
            "Downloading model.safetensors:  88%|████████▊ | 3.48G/3.94G [00:17<00:01, 252MB/s]\n",
            "Downloading model.safetensors:  89%|████████▉ | 3.51G/3.94G [00:17<00:01, 252MB/s]\n",
            "Downloading model.safetensors:  90%|████████▉ | 3.54G/3.94G [00:17<00:01, 253MB/s]\n",
            "Downloading model.safetensors:  91%|█████████ | 3.58G/3.94G [00:17<00:01, 258MB/s]\n",
            "Downloading model.safetensors:  91%|█████████▏| 3.61G/3.94G [00:18<00:01, 249MB/s]\n",
            "Downloading model.safetensors:  92%|█████████▏| 3.64G/3.94G [00:18<00:01, 254MB/s]\n",
            "Downloading model.safetensors:  93%|█████████▎| 3.67G/3.94G [00:18<00:01, 243MB/s]\n",
            "Downloading model.safetensors:  94%|█████████▍| 3.70G/3.94G [00:18<00:00, 253MB/s]\n",
            "Downloading model.safetensors:  95%|█████████▍| 3.73G/3.94G [00:18<00:00, 250MB/s]\n",
            "Downloading model.safetensors:  95%|█████████▌| 3.76G/3.94G [00:18<00:00, 252MB/s]\n",
            "Downloading model.safetensors:  96%|█████████▌| 3.80G/3.94G [00:18<00:00, 252MB/s]\n",
            "Downloading model.safetensors:  97%|█████████▋| 3.83G/3.94G [00:18<00:00, 257MB/s]\n",
            "Downloading model.safetensors:  98%|█████████▊| 3.86G/3.94G [00:19<00:00, 257MB/s]\n",
            "Downloading model.safetensors:  99%|█████████▊| 3.89G/3.94G [00:19<00:00, 261MB/s]\n",
            "Downloading model.safetensors:  99%|█████████▉| 3.92G/3.94G [00:19<00:00, 263MB/s]\n",
            "Downloading model.safetensors: 100%|██████████| 3.94G/3.94G [00:19<00:00, 203MB/s]\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'logit_scale', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "\n",
            "Downloading (…)rocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]\n",
            "Downloading (…)rocessor_config.json: 100%|██████████| 316/316 [00:00<00:00, 1.87MB/s]\n",
            "\n",
            "Downloading tokenizer_config.json:   0%|          | 0.00/904 [00:00<?, ?B/s]\n",
            "Downloading tokenizer_config.json: 100%|██████████| 904/904 [00:00<00:00, 5.85MB/s]\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "\n",
            "Downloading vocab.json: 0.00B [00:00, ?B/s]\n",
            "Downloading vocab.json: 862kB [00:00, 26.7MB/s]\n",
            "\n",
            "Downloading merges.txt: 0.00B [00:00, ?B/s]\n",
            "Downloading merges.txt: 525kB [00:00, 98.8MB/s]\n",
            "\n",
            "Downloading tokenizer.json: 0.00B [00:00, ?B/s]\n",
            "Downloading tokenizer.json: 2.22MB [00:00, 133MB/s]\n",
            "\n",
            "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 389/389 [00:00<00:00, 3.28MB/s]\n",
            "\n",
            "Downloading vocab.json: 0.00B [00:00, ?B/s]\n",
            "Downloading vocab.json: 1.06MB [00:00, 157MB/s]\n",
            "\n",
            "Downloading merges.txt: 0.00B [00:00, ?B/s]\n",
            "Downloading merges.txt: 525kB [00:00, 107MB/s]\n",
            "\n",
            "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]\n",
            "Downloading (…)cial_tokens_map.json: 100%|██████████| 460/460 [00:00<00:00, 3.86MB/s]\n",
            "\n",
            "Downloading tokenizer_config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]\n",
            "Downloading tokenizer_config.json: 100%|██████████| 829/829 [00:00<00:00, 6.76MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/hub.py:286: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n",
            "Downloading: \"https://github.com/miccunifi/ladi-vton/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "\n",
            "Downloading config.json:   0%|          | 0.00/914 [00:00<?, ?B/s]\n",
            "Downloading config.json: 100%|██████████| 914/914 [00:00<00:00, 5.57MB/s]\n",
            "Downloading: \"https://github.com/miccunifi/ladi-vton/releases/download/weights/unet_dresscode.pth\" to /root/.cache/torch/hub/checkpoints/unet_dresscode.pth\n",
            "\n",
            "  0%|          | 0.00/1.61G [00:00<?, ?B/s]\n",
            "  1%|          | 10.0M/1.61G [00:00<00:20, 85.3MB/s]\n",
            "  1%|▏         | 24.8M/1.61G [00:00<00:13, 123MB/s] \n",
            "  2%|▏         | 40.0M/1.61G [00:00<00:14, 120MB/s]\n",
            "  3%|▎         | 56.1M/1.61G [00:00<00:12, 137MB/s]\n",
            "  4%|▍         | 70.0M/1.61G [00:00<00:13, 124MB/s]\n",
            "  5%|▍         | 82.2M/1.61G [00:00<00:13, 121MB/s]\n",
            "  6%|▌         | 100M/1.61G [00:00<00:13, 125MB/s] \n",
            "  7%|▋         | 115M/1.61G [00:00<00:11, 135MB/s]\n",
            "  8%|▊         | 130M/1.61G [00:01<00:12, 125MB/s]\n",
            "  9%|▉         | 147M/1.61G [00:01<00:11, 138MB/s]\n",
            " 10%|▉         | 161M/1.61G [00:01<00:12, 130MB/s]\n",
            " 11%|█         | 179M/1.61G [00:01<00:10, 148MB/s]\n",
            " 12%|█▏        | 194M/1.61G [00:01<00:11, 138MB/s]\n",
            " 13%|█▎        | 207M/1.61G [00:04<01:36, 15.7MB/s]\n",
            " 13%|█▎        | 218M/1.61G [00:04<01:15, 19.9MB/s]\n",
            " 14%|█▍        | 230M/1.61G [00:04<01:00, 24.8MB/s]\n",
            " 14%|█▍        | 239M/1.61G [00:05<00:52, 28.5MB/s]\n",
            " 15%|█▍        | 248M/1.61G [00:05<00:43, 33.7MB/s]\n",
            " 15%|█▌        | 256M/1.61G [00:05<00:39, 36.9MB/s]\n",
            " 16%|█▌        | 263M/1.61G [00:05<00:36, 40.4MB/s]\n",
            " 16%|█▋        | 270M/1.61G [00:05<00:32, 44.9MB/s]\n",
            " 17%|█▋        | 276M/1.61G [00:05<00:30, 47.4MB/s]\n",
            " 17%|█▋        | 283M/1.61G [00:05<00:29, 48.3MB/s]\n",
            " 18%|█▊        | 290M/1.61G [00:05<00:29, 48.0MB/s]\n",
            " 18%|█▊        | 298M/1.61G [00:06<00:25, 54.6MB/s]\n",
            " 18%|█▊        | 304M/1.61G [00:06<00:27, 51.7MB/s]\n",
            " 19%|█▉        | 310M/1.61G [00:06<00:28, 48.8MB/s]\n",
            " 19%|█▉        | 318M/1.61G [00:06<00:25, 55.6MB/s]\n",
            " 20%|█▉        | 324M/1.61G [00:06<00:31, 43.8MB/s]\n",
            " 20%|█▉        | 330M/1.61G [00:06<00:32, 43.3MB/s]\n",
            " 20%|██        | 337M/1.61G [00:06<00:27, 50.0MB/s]\n",
            " 21%|██        | 343M/1.61G [00:07<00:27, 50.3MB/s]\n",
            " 21%|██        | 349M/1.61G [00:07<00:24, 55.7MB/s]\n",
            " 21%|██▏       | 355M/1.61G [00:07<00:24, 54.6MB/s]\n",
            " 22%|██▏       | 361M/1.61G [00:07<00:25, 52.9MB/s]\n",
            " 22%|██▏       | 368M/1.61G [00:07<00:22, 59.4MB/s]\n",
            " 23%|██▎       | 374M/1.61G [00:07<00:39, 34.2MB/s]\n",
            " 23%|██▎       | 380M/1.61G [00:07<00:37, 35.7MB/s]\n",
            " 23%|██▎       | 388M/1.61G [00:08<00:29, 44.6MB/s]\n",
            " 24%|██▍       | 394M/1.61G [00:08<00:28, 46.2MB/s]\n",
            " 24%|██▍       | 400M/1.61G [00:08<00:28, 46.7MB/s]\n",
            " 25%|██▍       | 405M/1.61G [00:08<00:35, 37.1MB/s]\n",
            " 25%|██▍       | 410M/1.61G [00:08<00:34, 37.5MB/s]\n",
            " 25%|██▌       | 417M/1.61G [00:08<00:28, 45.8MB/s]\n",
            " 26%|██▌       | 422M/1.61G [00:08<00:27, 46.6MB/s]\n",
            " 26%|██▌       | 430M/1.61G [00:09<00:23, 53.8MB/s]\n",
            " 26%|██▋       | 435M/1.61G [00:09<00:24, 51.7MB/s]\n",
            " 27%|██▋       | 440M/1.61G [00:09<00:25, 49.7MB/s]\n",
            " 27%|██▋       | 448M/1.61G [00:09<00:22, 56.2MB/s]\n",
            " 27%|██▋       | 454M/1.61G [00:09<00:22, 54.9MB/s]\n",
            " 28%|██▊       | 460M/1.61G [00:09<00:24, 51.6MB/s]\n",
            " 28%|██▊       | 468M/1.61G [00:09<00:21, 58.7MB/s]\n",
            " 29%|██▊       | 474M/1.61G [00:09<00:21, 56.2MB/s]\n",
            " 29%|██▉       | 480M/1.61G [00:10<00:23, 52.8MB/s]\n",
            " 30%|██▉       | 488M/1.61G [00:10<00:20, 59.1MB/s]\n",
            " 30%|██▉       | 494M/1.61G [00:10<00:34, 35.1MB/s]\n",
            " 30%|███       | 500M/1.61G [00:10<00:31, 38.0MB/s]\n",
            " 31%|███       | 508M/1.61G [00:10<00:25, 46.7MB/s]\n",
            " 31%|███       | 514M/1.61G [00:10<00:25, 46.9MB/s]\n",
            " 31%|███▏      | 520M/1.61G [00:10<00:24, 48.4MB/s]\n",
            " 32%|███▏      | 528M/1.61G [00:11<00:20, 56.4MB/s]\n",
            " 32%|███▏      | 534M/1.61G [00:11<00:21, 54.9MB/s]\n",
            " 33%|███▎      | 540M/1.61G [00:11<00:23, 49.9MB/s]\n",
            " 33%|███▎      | 548M/1.61G [00:11<00:20, 55.9MB/s]\n",
            " 34%|███▎      | 554M/1.61G [00:11<00:21, 54.7MB/s]\n",
            " 34%|███▍      | 560M/1.61G [00:11<00:21, 54.0MB/s]\n",
            " 34%|███▍      | 568M/1.61G [00:11<00:18, 60.9MB/s]\n",
            " 35%|███▍      | 574M/1.61G [00:11<00:19, 58.3MB/s]\n",
            " 35%|███▌      | 580M/1.61G [00:12<00:20, 55.3MB/s]\n",
            " 36%|███▌      | 588M/1.61G [00:12<00:18, 60.1MB/s]\n",
            " 36%|███▌      | 594M/1.61G [00:12<00:19, 57.1MB/s]\n",
            " 36%|███▋      | 599M/1.61G [00:12<00:30, 35.8MB/s]\n",
            " 37%|███▋      | 604M/1.61G [00:12<00:29, 36.8MB/s]\n",
            " 37%|███▋      | 610M/1.61G [00:12<00:27, 40.4MB/s]\n",
            " 37%|███▋      | 617M/1.61G [00:12<00:22, 47.7MB/s]\n",
            " 38%|███▊      | 622M/1.61G [00:13<00:22, 48.9MB/s]\n",
            " 38%|███▊      | 629M/1.61G [00:13<00:19, 55.4MB/s]\n",
            " 38%|███▊      | 635M/1.61G [00:13<00:19, 54.0MB/s]\n",
            " 39%|███▉      | 641M/1.61G [00:13<00:20, 52.6MB/s]\n",
            " 39%|███▉      | 646M/1.61G [00:13<00:29, 36.1MB/s]\n",
            " 39%|███▉      | 650M/1.61G [00:13<00:28, 36.9MB/s]\n",
            " 40%|███▉      | 658M/1.61G [00:13<00:22, 47.2MB/s]\n",
            " 40%|████      | 663M/1.61G [00:14<00:24, 41.6MB/s]\n",
            " 41%|████      | 670M/1.61G [00:14<00:23, 43.8MB/s]\n",
            " 41%|████      | 678M/1.61G [00:14<00:19, 52.5MB/s]\n",
            " 41%|████▏     | 684M/1.61G [00:14<00:18, 53.8MB/s]\n",
            " 42%|████▏     | 690M/1.61G [00:14<00:20, 50.2MB/s]\n",
            " 42%|████▏     | 695M/1.61G [00:14<00:28, 35.0MB/s]\n",
            " 42%|████▏     | 700M/1.61G [00:14<00:26, 37.0MB/s]\n",
            " 43%|████▎     | 708M/1.61G [00:15<00:21, 46.2MB/s]\n",
            " 43%|████▎     | 713M/1.61G [00:15<00:20, 47.6MB/s]\n",
            " 44%|████▎     | 720M/1.61G [00:15<00:18, 53.5MB/s]\n",
            " 44%|████▍     | 725M/1.61G [00:15<00:21, 45.5MB/s]\n",
            " 44%|████▍     | 730M/1.61G [00:15<00:21, 44.1MB/s]\n",
            " 45%|████▍     | 738M/1.61G [00:15<00:18, 51.0MB/s]\n",
            " 45%|████▍     | 743M/1.61G [00:16<00:27, 34.1MB/s]\n",
            " 45%|████▌     | 749M/1.61G [00:16<00:24, 38.2MB/s]\n",
            " 46%|████▌     | 754M/1.61G [00:16<00:23, 40.8MB/s]\n",
            " 46%|████▌     | 760M/1.61G [00:16<00:21, 42.8MB/s]\n",
            " 46%|████▋     | 768M/1.61G [00:16<00:17, 51.9MB/s]\n",
            " 47%|████▋     | 774M/1.61G [00:16<00:17, 51.3MB/s]\n",
            " 47%|████▋     | 780M/1.61G [00:16<00:18, 50.0MB/s]\n",
            " 48%|████▊     | 788M/1.61G [00:16<00:15, 57.9MB/s]\n",
            " 48%|████▊     | 794M/1.61G [00:16<00:16, 55.7MB/s]\n",
            " 48%|████▊     | 800M/1.61G [00:17<00:16, 53.9MB/s]\n",
            " 49%|████▉     | 808M/1.61G [00:17<00:14, 59.8MB/s]\n",
            " 49%|████▉     | 814M/1.61G [00:17<00:15, 56.9MB/s]\n",
            " 50%|████▉     | 820M/1.61G [00:17<00:18, 47.1MB/s]\n",
            " 50%|████▉     | 825M/1.61G [00:19<01:43, 8.39MB/s]\n",
            " 51%|█████     | 835M/1.61G [00:19<01:03, 13.6MB/s]\n",
            " 51%|█████     | 842M/1.61G [00:19<00:47, 17.7MB/s]\n",
            " 51%|█████▏    | 848M/1.61G [00:19<00:37, 22.2MB/s]\n",
            " 52%|█████▏    | 854M/1.61G [00:20<00:32, 25.6MB/s]\n",
            " 52%|█████▏    | 859M/1.61G [00:20<00:33, 24.9MB/s]\n",
            " 52%|█████▏    | 864M/1.61G [00:20<00:30, 27.2MB/s]\n",
            " 53%|█████▎    | 870M/1.61G [00:20<00:26, 31.4MB/s]\n",
            " 53%|█████▎    | 878M/1.61G [00:20<00:19, 40.9MB/s]\n",
            " 53%|█████▎    | 884M/1.61G [00:20<00:20, 39.9MB/s]\n",
            " 54%|█████▍    | 890M/1.61G [00:21<00:23, 34.3MB/s]\n",
            " 54%|█████▍    | 898M/1.61G [00:21<00:18, 42.9MB/s]\n",
            " 55%|█████▍    | 903M/1.61G [00:21<00:18, 42.8MB/s]\n",
            " 55%|█████▌    | 910M/1.61G [00:21<00:17, 44.4MB/s]\n",
            " 56%|█████▌    | 918M/1.61G [00:21<00:14, 52.7MB/s]\n",
            " 56%|█████▌    | 924M/1.61G [00:21<00:15, 50.6MB/s]\n",
            " 56%|█████▋    | 930M/1.61G [00:21<00:15, 49.7MB/s]\n",
            " 57%|█████▋    | 937M/1.61G [00:21<00:13, 55.8MB/s]\n",
            " 57%|█████▋    | 943M/1.61G [00:22<00:14, 52.2MB/s]\n",
            " 58%|█████▊    | 950M/1.61G [00:22<00:14, 50.9MB/s]\n",
            " 58%|█████▊    | 955M/1.61G [00:23<00:49, 14.8MB/s]\n",
            " 59%|█████▉    | 972M/1.61G [00:23<00:24, 29.0MB/s]\n",
            " 59%|█████▉    | 979M/1.61G [00:23<00:20, 35.0MB/s]\n",
            " 60%|█████▉    | 987M/1.61G [00:23<00:17, 39.3MB/s]\n",
            " 60%|██████    | 994M/1.61G [00:23<00:16, 42.9MB/s]\n",
            " 61%|██████    | 0.98G/1.61G [00:24<00:16, 41.8MB/s]\n",
            " 61%|██████    | 0.98G/1.61G [00:24<00:21, 32.2MB/s]\n",
            " 61%|██████    | 0.99G/1.61G [00:24<00:19, 33.7MB/s]\n",
            " 61%|██████▏   | 0.99G/1.61G [00:24<00:28, 23.1MB/s]\n",
            " 62%|██████▏   | 1.00G/1.61G [00:25<00:23, 28.2MB/s]\n",
            " 62%|██████▏   | 1.00G/1.61G [00:25<00:20, 32.6MB/s]\n",
            " 62%|██████▏   | 1.01G/1.61G [00:25<00:17, 36.5MB/s]\n",
            " 63%|██████▎   | 1.01G/1.61G [00:25<00:16, 39.6MB/s]\n",
            " 63%|██████▎   | 1.02G/1.61G [00:25<00:15, 41.6MB/s]\n",
            " 63%|██████▎   | 1.02G/1.61G [00:25<00:14, 42.6MB/s]\n",
            " 64%|██████▎   | 1.03G/1.61G [00:25<00:14, 42.5MB/s]\n",
            " 64%|██████▍   | 1.03G/1.61G [00:25<00:18, 34.1MB/s]\n",
            " 64%|██████▍   | 1.04G/1.61G [00:25<00:15, 39.6MB/s]\n",
            " 64%|██████▍   | 1.04G/1.61G [00:27<01:24, 7.32MB/s]\n",
            " 65%|██████▍   | 1.04G/1.61G [00:38<01:23, 7.32MB/s]\n",
            " 65%|██████▍   | 1.04G/1.61G [00:40<10:03, 1.02MB/s]\n",
            " 65%|██████▍   | 1.04G/1.61G [00:40<09:21, 1.09MB/s]\n",
            " 65%|██████▍   | 1.05G/1.61G [00:41<07:28, 1.36MB/s]\n",
            " 65%|██████▍   | 1.05G/1.61G [00:41<06:13, 1.63MB/s]\n",
            " 65%|██████▍   | 1.05G/1.61G [00:41<05:18, 1.90MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:41<04:35, 2.20MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:42<04:05, 2.47MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:42<03:37, 2.77MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:42<03:18, 3.04MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:42<03:02, 3.30MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:42<02:54, 3.45MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:42<02:51, 3.51MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:43<02:57, 3.37MB/s]\n",
            " 65%|██████▌   | 1.05G/1.61G [00:43<03:21, 2.97MB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:43<04:19, 2.31MB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:43<04:41, 2.13MB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:44<05:40, 1.76MB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:44<08:24, 1.19MB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:44<11:07, 896kB/s] \n",
            " 65%|██████▌   | 1.06G/1.61G [00:45<19:13, 518kB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:45<18:32, 538kB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:46<17:47, 560kB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:46<16:51, 591kB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:46<17:06, 582kB/s]\n",
            " 65%|██████▌   | 1.06G/1.61G [00:46<16:37, 599kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:46<16:22, 608kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:46<16:11, 615kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:47<17:23, 573kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:47<16:52, 590kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:47<17:56, 555kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:47<18:45, 530kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:47<19:23, 513kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:48<19:51, 501kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:48<20:11, 492kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:48<20:26, 486kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:48<20:36, 482kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:49<20:44, 479kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:49<20:49, 477kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:49<20:52, 476kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:49<23:13, 428kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:49<22:30, 441kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:50<22:02, 451kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:50<21:43, 457kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:50<21:29, 462kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:50<21:20, 465kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:50<21:14, 467kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:51<21:09, 469kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:51<23:28, 423kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:51<22:39, 438kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:51<22:07, 448kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:51<21:46, 456kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:52<21:31, 461kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:52<23:46, 417kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:52<22:51, 434kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:52<22:15, 445kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:52<21:51, 454kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:53<21:34, 459kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:53<19:51, 499kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:53<20:10, 491kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:53<20:24, 485kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:53<18:25, 537kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:54<15:55, 622kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:54<13:48, 717kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:54<12:38, 783kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:54<11:41, 846kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:54<11:27, 863kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:54<11:03, 894kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:54<10:59, 899kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:55<10:57, 902kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:55<10:55, 905kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:55<09:59, 989kB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:55<08:51, 1.12MB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:55<08:19, 1.19MB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:55<08:20, 1.18MB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:55<08:13, 1.20MB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:56<08:08, 1.21MB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:56<08:14, 1.20MB/s]\n",
            " 66%|██████▌   | 1.06G/1.61G [00:56<08:14, 1.20MB/s]\n",
            " 66%|██████▌   | 1.07G/1.61G [00:56<00:44, 13.2MB/s]\n",
            " 66%|██████▋   | 1.07G/1.61G [00:56<00:29, 20.0MB/s]\n",
            " 67%|██████▋   | 1.07G/1.61G [00:56<00:24, 23.6MB/s]\n",
            " 67%|██████▋   | 1.08G/1.61G [00:56<00:20, 28.7MB/s]\n",
            " 67%|██████▋   | 1.08G/1.61G [00:56<00:16, 35.4MB/s]\n",
            " 67%|██████▋   | 1.09G/1.61G [00:56<00:14, 38.5MB/s]\n",
            " 68%|██████▊   | 1.09G/1.61G [00:56<00:13, 40.6MB/s]\n",
            " 68%|██████▊   | 1.09G/1.61G [00:57<00:14, 38.2MB/s]\n",
            " 68%|██████▊   | 1.10G/1.61G [00:57<00:15, 36.3MB/s]\n",
            " 69%|██████▊   | 1.11G/1.61G [00:57<00:11, 47.2MB/s]\n",
            " 69%|██████▉   | 1.11G/1.61G [00:57<00:09, 54.8MB/s]\n",
            " 70%|██████▉   | 1.12G/1.61G [00:57<00:07, 68.0MB/s]\n",
            " 70%|███████   | 1.13G/1.61G [00:57<00:07, 71.0MB/s]\n",
            " 71%|███████   | 1.14G/1.61G [00:57<00:06, 73.9MB/s]\n",
            " 71%|███████   | 1.15G/1.61G [00:58<00:13, 35.9MB/s]\n",
            " 71%|███████▏  | 1.15G/1.61G [01:03<01:50, 4.48MB/s]\n",
            " 72%|███████▏  | 1.16G/1.61G [01:03<01:05, 7.41MB/s]\n",
            " 73%|███████▎  | 1.17G/1.61G [01:03<00:42, 11.0MB/s]\n",
            " 73%|███████▎  | 1.18G/1.61G [01:03<00:36, 12.5MB/s]\n",
            " 74%|███████▎  | 1.19G/1.61G [01:03<00:30, 15.1MB/s]\n",
            " 74%|███████▍  | 1.19G/1.61G [01:04<00:25, 18.0MB/s]\n",
            " 74%|███████▍  | 1.20G/1.61G [01:04<00:19, 23.2MB/s]\n",
            " 75%|███████▍  | 1.21G/1.61G [01:04<00:21, 20.4MB/s]\n",
            " 75%|███████▌  | 1.21G/1.61G [01:04<00:17, 24.1MB/s]\n",
            " 76%|███████▌  | 1.22G/1.61G [01:04<00:13, 31.0MB/s]\n",
            " 76%|███████▌  | 1.22G/1.61G [01:05<00:15, 27.8MB/s]\n",
            " 76%|███████▌  | 1.23G/1.61G [01:07<00:53, 7.69MB/s]\n",
            " 76%|███████▋  | 1.23G/1.61G [01:07<00:51, 8.02MB/s]\n",
            " 77%|███████▋  | 1.25G/1.61G [01:07<00:18, 20.6MB/s]\n",
            " 78%|███████▊  | 1.26G/1.61G [01:07<00:14, 26.0MB/s]\n",
            " 78%|███████▊  | 1.27G/1.61G [01:07<00:12, 29.6MB/s]\n",
            " 79%|███████▉  | 1.27G/1.61G [01:07<00:11, 32.6MB/s]\n",
            " 79%|███████▉  | 1.28G/1.61G [01:08<00:09, 39.4MB/s]\n",
            " 80%|███████▉  | 1.29G/1.61G [01:08<00:08, 41.3MB/s]\n",
            " 80%|████████  | 1.29G/1.61G [01:08<00:07, 43.4MB/s]\n",
            " 80%|████████  | 1.30G/1.61G [01:08<00:06, 49.0MB/s]\n",
            " 81%|████████  | 1.30G/1.61G [01:08<00:09, 33.9MB/s]\n",
            " 81%|████████  | 1.31G/1.61G [01:13<01:22, 3.98MB/s]\n",
            " 82%|████████▏ | 1.33G/1.61G [01:13<00:31, 9.57MB/s]\n",
            " 83%|████████▎ | 1.34G/1.61G [01:13<00:24, 12.2MB/s]\n",
            " 83%|████████▎ | 1.34G/1.61G [01:13<00:19, 15.1MB/s]\n",
            " 84%|████████▎ | 1.35G/1.61G [01:14<00:15, 18.1MB/s]\n",
            " 84%|████████▍ | 1.36G/1.61G [01:14<00:12, 21.5MB/s]\n",
            " 85%|████████▍ | 1.37G/1.61G [01:14<00:09, 27.6MB/s]\n",
            " 85%|████████▌ | 1.37G/1.61G [01:14<00:08, 31.4MB/s]\n",
            " 85%|████████▌ | 1.38G/1.61G [01:14<00:07, 35.1MB/s]\n",
            " 86%|████████▌ | 1.38G/1.61G [01:15<00:12, 20.1MB/s]\n",
            " 86%|████████▌ | 1.39G/1.61G [01:19<01:01, 3.94MB/s]\n",
            " 87%|████████▋ | 1.41G/1.61G [01:19<00:23, 9.36MB/s]\n",
            " 88%|████████▊ | 1.41G/1.61G [01:20<00:19, 11.0MB/s]\n",
            " 88%|████████▊ | 1.42G/1.61G [01:20<00:15, 13.2MB/s]\n",
            " 88%|████████▊ | 1.43G/1.61G [01:20<00:12, 15.9MB/s]\n",
            " 89%|████████▉ | 1.43G/1.61G [01:20<00:09, 19.7MB/s]\n",
            " 89%|████████▉ | 1.44G/1.61G [01:20<00:07, 24.0MB/s]\n",
            " 90%|████████▉ | 1.44G/1.61G [01:20<00:06, 29.5MB/s]\n",
            " 90%|████████▉ | 1.45G/1.61G [01:20<00:05, 33.3MB/s]\n",
            " 90%|█████████ | 1.46G/1.61G [01:20<00:04, 35.9MB/s]\n",
            " 91%|█████████ | 1.46G/1.61G [01:21<00:04, 36.0MB/s]\n",
            " 91%|█████████ | 1.47G/1.61G [01:24<00:34, 4.66MB/s]\n",
            " 91%|█████████ | 1.47G/1.61G [01:25<00:35, 4.43MB/s]\n",
            " 92%|█████████▏| 1.49G/1.61G [01:25<00:11, 11.8MB/s]\n",
            " 93%|█████████▎| 1.49G/1.61G [01:26<00:08, 14.2MB/s]\n",
            " 93%|█████████▎| 1.50G/1.61G [01:26<00:06, 18.4MB/s]\n",
            " 93%|█████████▎| 1.51G/1.61G [01:26<00:05, 22.1MB/s]\n",
            " 94%|█████████▍| 1.51G/1.61G [01:26<00:04, 25.3MB/s]\n",
            " 94%|█████████▍| 1.52G/1.61G [01:26<00:03, 31.8MB/s]\n",
            " 95%|█████████▍| 1.53G/1.61G [01:26<00:02, 36.4MB/s]\n",
            " 95%|█████████▌| 1.53G/1.61G [01:26<00:02, 37.2MB/s]\n",
            " 95%|█████████▌| 1.54G/1.61G [01:26<00:01, 42.2MB/s]\n",
            " 96%|█████████▌| 1.54G/1.61G [01:30<00:14, 5.20MB/s]\n",
            " 96%|█████████▌| 1.55G/1.61G [01:31<00:14, 4.68MB/s]\n",
            " 97%|█████████▋| 1.57G/1.61G [01:32<00:04, 10.7MB/s]\n",
            " 97%|█████████▋| 1.57G/1.61G [01:32<00:03, 12.9MB/s]\n",
            " 98%|█████████▊| 1.58G/1.61G [01:32<00:02, 16.3MB/s]\n",
            " 98%|█████████▊| 1.58G/1.61G [01:32<00:01, 19.7MB/s]\n",
            " 99%|█████████▊| 1.59G/1.61G [01:32<00:00, 24.7MB/s]\n",
            " 99%|█████████▉| 1.60G/1.61G [01:32<00:00, 21.6MB/s]\n",
            " 99%|█████████▉| 1.60G/1.61G [01:33<00:00, 24.1MB/s]\n",
            "100%|█████████▉| 1.61G/1.61G [01:33<00:00, 31.3MB/s]\n",
            "100%|██████████| 1.61G/1.61G [01:34<00:00, 18.4MB/s]\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Downloading: \"https://github.com/miccunifi/ladi-vton/releases/download/weights/emasc_dresscode.pth\" to /root/.cache/torch/hub/checkpoints/emasc_dresscode.pth\n",
            "\n",
            "  0%|          | 0.00/30.4M [00:00<?, ?B/s]\n",
            " 26%|██▋       | 8.02M/30.4M [00:00<00:00, 80.3MB/s]\n",
            " 52%|█████▏    | 15.7M/30.4M [00:00<00:00, 65.7MB/s]\n",
            " 73%|███████▎  | 22.1M/30.4M [00:00<00:00, 59.3MB/s]\n",
            " 97%|█████████▋| 29.5M/30.4M [00:00<00:00, 65.4MB/s]\n",
            "100%|██████████| 30.4M/30.4M [00:00<00:00, 59.1MB/s]\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Downloading: \"https://github.com/miccunifi/ladi-vton/releases/download/weights/inversion_adapter_dresscode.pth\" to /root/.cache/torch/hub/checkpoints/inversion_adapter_dresscode.pth\n",
            "\n",
            "  0%|          | 0.00/520M [00:00<?, ?B/s]\n",
            "  1%|          | 4.00M/520M [00:00<00:21, 25.0MB/s]\n",
            "  1%|          | 6.39M/520M [00:00<00:29, 18.3MB/s]\n",
            "  2%|▏         | 10.0M/520M [00:00<00:23, 23.2MB/s]\n",
            "  4%|▎         | 18.2M/520M [00:00<00:13, 40.0MB/s]\n",
            "  4%|▍         | 22.5M/520M [00:00<00:12, 41.4MB/s]\n",
            "  6%|▌         | 30.0M/520M [00:00<00:11, 44.9MB/s]\n",
            "  7%|▋         | 38.3M/520M [00:00<00:09, 55.5MB/s]\n",
            "  8%|▊         | 43.9M/520M [00:01<00:09, 52.7MB/s]\n",
            "  9%|▉         | 49.2M/520M [00:01<00:09, 53.4MB/s]\n",
            " 10%|█         | 54.4M/520M [00:01<00:15, 32.1MB/s]\n",
            " 11%|█         | 58.5M/520M [00:01<00:21, 22.5MB/s]\n",
            " 12%|█▏        | 61.7M/520M [00:04<01:25, 5.60MB/s]\n",
            " 14%|█▎        | 71.4M/520M [00:04<00:46, 10.2MB/s]\n",
            " 15%|█▌        | 79.1M/520M [00:04<00:31, 14.7MB/s]\n",
            " 16%|█▋        | 84.5M/520M [00:04<00:25, 17.9MB/s]\n",
            " 17%|█▋        | 90.0M/520M [00:04<00:20, 21.7MB/s]\n",
            " 18%|█▊        | 95.0M/520M [00:04<00:22, 19.8MB/s]\n",
            " 19%|█▉        | 100M/520M [00:05<00:21, 20.9MB/s] \n",
            " 20%|█▉        | 104M/520M [00:05<00:18, 23.3MB/s]\n",
            " 21%|██        | 107M/520M [00:05<00:19, 22.7MB/s]\n",
            " 21%|██        | 111M/520M [00:05<00:17, 24.0MB/s]\n",
            " 22%|██▏       | 114M/520M [00:05<00:17, 24.1MB/s]\n",
            " 22%|██▏       | 117M/520M [00:05<00:19, 21.2MB/s]\n",
            " 23%|██▎       | 120M/520M [00:05<00:17, 23.5MB/s]\n",
            " 24%|██▍       | 124M/520M [00:06<00:15, 26.8MB/s]\n",
            " 24%|██▍       | 127M/520M [00:06<00:19, 21.7MB/s]\n",
            " 25%|██▍       | 129M/520M [00:06<00:23, 17.8MB/s]\n",
            " 25%|██▌       | 131M/520M [00:06<00:28, 14.3MB/s]\n",
            " 26%|██▌       | 133M/520M [00:07<00:43, 9.27MB/s]\n",
            " 26%|██▌       | 134M/520M [00:09<02:50, 2.38MB/s]\n",
            " 26%|██▌       | 135M/520M [00:10<02:54, 2.32MB/s]\n",
            " 26%|██▌       | 136M/520M [00:10<02:42, 2.47MB/s]\n",
            " 29%|██▉       | 150M/520M [00:10<00:32, 12.0MB/s]\n",
            " 30%|███       | 158M/520M [00:10<00:20, 18.2MB/s]\n",
            " 31%|███▏      | 163M/520M [00:10<00:17, 21.8MB/s]\n",
            " 33%|███▎      | 170M/520M [00:10<00:14, 26.0MB/s]\n",
            " 34%|███▍      | 178M/520M [00:10<00:10, 33.7MB/s]\n",
            " 35%|███▌      | 184M/520M [00:11<00:09, 37.5MB/s]\n",
            " 36%|███▋      | 189M/520M [00:11<00:08, 39.4MB/s]\n",
            " 37%|███▋      | 194M/520M [00:11<00:13, 26.1MB/s]\n",
            " 38%|███▊      | 198M/520M [00:12<00:25, 13.3MB/s]\n",
            " 39%|███▊      | 201M/520M [00:16<01:46, 3.15MB/s]\n",
            " 42%|████▏     | 220M/520M [00:16<00:37, 8.51MB/s]\n",
            " 44%|████▍     | 228M/520M [00:16<00:26, 11.4MB/s]\n",
            " 45%|████▌     | 235M/520M [00:16<00:21, 14.0MB/s]\n",
            " 46%|████▌     | 240M/520M [00:17<00:19, 14.9MB/s]\n",
            " 47%|████▋     | 245M/520M [00:21<01:08, 4.22MB/s]\n",
            " 48%|████▊     | 248M/520M [00:22<01:13, 3.87MB/s]\n",
            " 50%|█████     | 261M/520M [00:22<00:36, 7.40MB/s]\n",
            " 51%|█████▏    | 267M/520M [00:22<00:28, 9.47MB/s]\n",
            " 53%|█████▎    | 274M/520M [00:22<00:20, 12.7MB/s]\n",
            " 54%|█████▍    | 280M/520M [00:23<00:15, 15.8MB/s]\n",
            " 55%|█████▌    | 288M/520M [00:23<00:11, 21.7MB/s]\n",
            " 57%|█████▋    | 294M/520M [00:23<00:09, 25.4MB/s]\n",
            " 58%|█████▊    | 300M/520M [00:23<00:08, 26.2MB/s]\n",
            " 59%|█████▊    | 304M/520M [00:24<00:14, 15.5MB/s]\n",
            " 59%|█████▉    | 308M/520M [00:26<00:41, 5.36MB/s]\n",
            " 62%|██████▏   | 321M/520M [00:26<00:19, 10.5MB/s]\n",
            " 63%|██████▎   | 328M/520M [00:26<00:14, 14.2MB/s]\n",
            " 64%|██████▍   | 334M/520M [00:27<00:13, 14.6MB/s]\n",
            " 65%|██████▌   | 340M/520M [00:27<00:10, 17.7MB/s]\n",
            " 67%|██████▋   | 348M/520M [00:27<00:07, 24.1MB/s]\n",
            " 68%|██████▊   | 353M/520M [00:27<00:06, 28.1MB/s]\n",
            " 69%|██████▉   | 360M/520M [00:27<00:05, 31.4MB/s]\n",
            " 70%|███████   | 366M/520M [00:27<00:04, 35.5MB/s]\n",
            " 71%|███████▏  | 371M/520M [00:28<00:06, 25.1MB/s]\n",
            " 72%|███████▏  | 375M/520M [00:32<00:40, 3.75MB/s]\n",
            " 73%|███████▎  | 378M/520M [00:32<00:33, 4.44MB/s]\n",
            " 75%|███████▌  | 392M/520M [00:32<00:13, 9.96MB/s]\n",
            " 77%|███████▋  | 400M/520M [00:33<00:09, 13.5MB/s]\n",
            " 78%|███████▊  | 406M/520M [00:33<00:07, 17.0MB/s]\n",
            " 79%|███████▉  | 412M/520M [00:33<00:05, 20.7MB/s]\n",
            " 81%|████████  | 419M/520M [00:33<00:04, 26.3MB/s]\n",
            " 82%|████████▏ | 425M/520M [00:33<00:03, 30.3MB/s]\n",
            " 83%|████████▎ | 431M/520M [00:33<00:02, 33.4MB/s]\n",
            " 84%|████████▍ | 438M/520M [00:33<00:02, 40.3MB/s]\n",
            " 85%|████████▌ | 444M/520M [00:33<00:01, 42.4MB/s]\n",
            " 86%|████████▋ | 449M/520M [00:34<00:02, 31.1MB/s]\n",
            " 87%|████████▋ | 454M/520M [00:34<00:02, 33.0MB/s]\n",
            " 88%|████████▊ | 460M/520M [00:34<00:01, 36.3MB/s]\n",
            " 90%|████████▉ | 467M/520M [00:34<00:01, 44.7MB/s]\n",
            " 91%|█████████ | 473M/520M [00:34<00:01, 45.4MB/s]\n",
            " 92%|█████████▏| 478M/520M [00:34<00:00, 47.6MB/s]\n",
            " 93%|█████████▎| 483M/520M [00:35<00:01, 21.0MB/s]\n",
            " 94%|█████████▎| 487M/520M [00:36<00:03, 9.97MB/s]\n",
            " 95%|█████████▍| 493M/520M [00:36<00:02, 14.1MB/s]\n",
            " 96%|█████████▌| 499M/520M [00:36<00:01, 19.0MB/s]\n",
            " 97%|█████████▋| 506M/520M [00:36<00:00, 24.7MB/s]\n",
            " 98%|█████████▊| 512M/520M [00:36<00:00, 30.7MB/s]\n",
            "100%|██████████| 520M/520M [00:37<00:00, 14.7MB/s]\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Downloading: \"https://github.com/miccunifi/ladi-vton/releases/download/weights/warping_dresscode.pth\" to /root/.cache/torch/hub/checkpoints/warping_dresscode.pth\n",
            "\n",
            "  0%|          | 0.00/276M [00:00<?, ?B/s]\n",
            "  3%|▎         | 7.98M/276M [00:00<00:03, 83.6MB/s]\n",
            "  6%|▌         | 16.0M/276M [00:00<00:04, 59.5MB/s]\n",
            "  8%|▊         | 22.0M/276M [00:00<00:04, 56.7MB/s]\n",
            " 11%|█         | 29.2M/276M [00:00<00:04, 63.0MB/s]\n",
            " 13%|█▎        | 35.5M/276M [00:00<00:06, 40.4MB/s]\n",
            " 15%|█▍        | 40.3M/276M [00:03<00:34, 7.18MB/s]\n",
            " 23%|██▎       | 62.3M/276M [00:03<00:11, 18.7MB/s]\n",
            " 26%|██▌       | 71.7M/276M [00:03<00:09, 23.0MB/s]\n",
            " 29%|██▉       | 79.8M/276M [00:03<00:07, 28.0MB/s]\n",
            " 32%|███▏      | 87.6M/276M [00:03<00:06, 32.4MB/s]\n",
            " 34%|███▍      | 94.8M/276M [00:04<00:10, 18.3MB/s]\n",
            " 36%|███▌      | 99.9M/276M [00:09<00:41, 4.49MB/s]\n",
            " 44%|████▎     | 120M/276M [00:09<00:17, 9.46MB/s] \n",
            " 47%|████▋     | 129M/276M [00:09<00:13, 11.7MB/s]\n",
            " 49%|████▉     | 136M/276M [00:09<00:11, 13.2MB/s]\n",
            " 51%|█████▏    | 142M/276M [00:09<00:09, 15.5MB/s]\n",
            " 53%|█████▎    | 147M/276M [00:10<00:07, 18.4MB/s]\n",
            " 55%|█████▌    | 153M/276M [00:13<00:25, 5.10MB/s]\n",
            " 57%|█████▋    | 156M/276M [00:15<00:29, 4.25MB/s]\n",
            " 63%|██████▎   | 174M/276M [00:15<00:11, 9.34MB/s]\n",
            " 65%|██████▌   | 180M/276M [00:15<00:08, 11.3MB/s]\n",
            " 68%|██████▊   | 187M/276M [00:15<00:06, 14.7MB/s]\n",
            " 70%|███████   | 194M/276M [00:16<00:05, 15.9MB/s]\n",
            " 73%|███████▎  | 200M/276M [00:16<00:04, 19.4MB/s]\n",
            " 75%|███████▌  | 208M/276M [00:16<00:02, 25.8MB/s]\n",
            " 77%|███████▋  | 214M/276M [00:20<00:12, 5.33MB/s]\n",
            " 79%|███████▉  | 218M/276M [00:21<00:13, 4.64MB/s]\n",
            " 84%|████████▍ | 231M/276M [00:21<00:05, 8.71MB/s]\n",
            " 86%|████████▌ | 237M/276M [00:21<00:04, 10.0MB/s]\n",
            " 88%|████████▊ | 242M/276M [00:22<00:03, 11.8MB/s]\n",
            " 90%|█████████ | 249M/276M [00:22<00:01, 16.1MB/s]\n",
            " 92%|█████████▏| 254M/276M [00:22<00:01, 18.7MB/s]\n",
            " 94%|█████████▍| 260M/276M [00:22<00:00, 21.7MB/s]\n",
            " 96%|█████████▌| 264M/276M [00:22<00:00, 23.8MB/s]\n",
            " 98%|█████████▊| 270M/276M [00:22<00:00, 27.4MB/s]\n",
            " 99%|█████████▉| 274M/276M [00:22<00:00, 26.9MB/s]\n",
            "100%|██████████| 276M/276M [00:24<00:00, 11.6MB/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:21,  2.32it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:18,  2.53it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:18,  2.59it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:17,  2.60it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:17,  2.63it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:16,  2.69it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:15,  2.73it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:15,  2.76it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:14,  2.79it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:14,  2.78it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:04<00:13,  2.79it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.80it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:13,  2.79it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:05<00:12,  2.78it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.79it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:12,  2.78it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:06<00:11,  2.79it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:11,  2.80it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:06<00:11,  2.79it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:10,  2.80it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.80it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:07<00:10,  2.80it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:09,  2.80it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.81it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:09<00:08,  2.80it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.80it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.80it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:10<00:07,  2.80it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.80it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:10<00:07,  2.79it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:06,  2.79it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.80it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:11<00:06,  2.77it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:05,  2.73it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.73it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:13<00:05,  2.72it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.70it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:13<00:04,  2.71it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:14<00:04,  2.70it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.69it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:14<00:03,  2.69it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:15<00:03,  2.66it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.66it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:16<00:02,  2.69it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:16<00:01,  2.71it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.74it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:17<00:01,  2.76it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:17<00:00,  2.76it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:17<00:00,  2.77it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.78it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.75it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:24<00:00, 24.87s/it]\n",
            "100%|██████████| 1/1 [00:24<00:00, 24.87s/it]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 3328.81it/s]\n",
            "\n",
            "custom stats: lower_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "custom stats: lower_body : 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
            "custom stats: lower_body : 100%|██████████| 1/1 [00:00<00:00,  3.13it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "custom stats: upper_body : : 0it [00:00, ?it/s]\n",
            "custom stats: upper_body : : 0it [00:00, ?it/s]\n",
            "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
            "│ /content/ladi-vton/src/inference.py:347 in <module>                          │\n",
            "│                                                                              │\n",
            "│   344                                                                        │\n",
            "│   345                                                                        │\n",
            "│   346 if __name__ == \"__main__\":                                             │\n",
            "│ ❱ 347 │   main()                                                             │\n",
            "│   348                                                                        │\n",
            "│                                                                              │\n",
            "│ /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:115 in    │\n",
            "│ decorate_context                                                             │\n",
            "│                                                                              │\n",
            "│   112 │   @functools.wraps(func)                                             │\n",
            "│   113 │   def decorate_context(*args, **kwargs):                             │\n",
            "│   114 │   │   with ctx_factory():                                            │\n",
            "│ ❱ 115 │   │   │   return func(*args, **kwargs)                               │\n",
            "│   116 │                                                                      │\n",
            "│   117 │   return decorate_context                                            │\n",
            "│   118                                                                        │\n",
            "│                                                                              │\n",
            "│ /content/ladi-vton/src/inference.py:339 in main                              │\n",
            "│                                                                              │\n",
            "│   336 │                                                                      │\n",
            "│   337 │   # Compute metrics if requested                                     │\n",
            "│   338 │   if args.compute_metrics:                                           │\n",
            "│ ❱ 339 │   │   metrics = compute_metrics(save_dir, args.test_order, args.data │\n",
            "│   340 │   │   │   │   │   │   │   │     args.dresscode_dataroot, args.vitonh │\n",
            "│   341 │   │                                                                  │\n",
            "│   342 │   │   with open(os.path.join(save_dir, f\"metrics_{args.test_order}_{ │\n",
            "│                                                                              │\n",
            "│ /content/ladi-vton/src/utils/val_metrics.py:156 in compute_metrics           │\n",
            "│                                                                              │\n",
            "│   153 │   │   │                                                              │\n",
            "│   154 │   │   │   # Check if FID stats exist, if not compute them            │\n",
            "│   155 │   │   │   if not fid.test_stats_exists(f\"{dataset}_{category}\", mode │\n",
            "│ ❱ 156 │   │   │   │   make_custom_stats(dresscode_dataroot, vitonhd_dataroot │\n",
            "│   157 │   │   │                                                              │\n",
            "│   158 │   │   │   # Compute FID score                                        │\n",
            "│   159 │   │   │   fid_score = fid.compute_fid(os.path.join(gen_folder, categ │\n",
            "│                                                                              │\n",
            "│ /content/ladi-vton/src/utils/generate_fid_stats.py:22 in make_custom_stats   │\n",
            "│                                                                              │\n",
            "│   19 │   │   │   │   os.makedirs(tmp_folder, exist_ok=True)                  │\n",
            "│   20 │   │   │   │   for path in tqdm(paths):                                │\n",
            "│   21 │   │   │   │   │   shutil.copy(path, tmp_folder)                       │\n",
            "│ ❱ 22 │   │   │   │   fid.make_custom_stats(f\"dresscode_{category}\", tmp_fold │\n",
            "│   23 │   │                                                                   │\n",
            "│   24 │   │   if not fid.test_stats_exists(f\"dresscode_all\", mode='clean'):   │\n",
            "│   25 │   │   │   paths = [os.path.join(dresscode_dataroot, category, 'images │\n",
            "│                                                                              │\n",
            "│ /usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:363 in               │\n",
            "│ make_custom_stats                                                            │\n",
            "│                                                                              │\n",
            "│   360 │   │   raise ValueError(f\"The entered model name - {model_name} was n │\n",
            "│   361 │                                                                      │\n",
            "│   362 │   # get all inception features for folder images                     │\n",
            "│ ❱ 363 │   np_feats = get_folder_features(fdir, feat_model, num_workers=num_w │\n",
            "│   364 │   │   │   │   │   │   │   │   │   batch_size=batch_size, device=devi │\n",
            "│   365 │   │   │   │   │   │   │   │   │   mode=mode, description=f\"custom st │\n",
            "│   366 │   │   │   │   │   │   │   │   │   custom_image_tranform=custom_image │\n",
            "│                                                                              │\n",
            "│ /usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:147 in               │\n",
            "│ get_folder_features                                                          │\n",
            "│                                                                              │\n",
            "│   144 │   │   │   random.seed(seed)                                          │\n",
            "│   145 │   │   │   random.shuffle(files)                                      │\n",
            "│   146 │   │   files = files[:num]                                            │\n",
            "│ ❱ 147 │   np_feats = get_files_features(files, model, num_workers=num_worker │\n",
            "│   148 │   │   │   │   │   │   │   │     batch_size=batch_size, device=device │\n",
            "│   149 │   │   │   │   │   │   │   │     custom_fn_resize=custom_fn_resize,   │\n",
            "│   150 │   │   │   │   │   │   │   │     custom_image_tranform=custom_image_t │\n",
            "│                                                                              │\n",
            "│ /usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:120 in               │\n",
            "│ get_files_features                                                           │\n",
            "│                                                                              │\n",
            "│   117 │                                                                      │\n",
            "│   118 │   for batch in pbar:                                                 │\n",
            "│   119 │   │   l_feats.append(get_batch_features(batch, model, device))       │\n",
            "│ ❱ 120 │   np_feats = np.concatenate(l_feats)                                 │\n",
            "│   121 │   return np_feats                                                    │\n",
            "│   122                                                                        │\n",
            "│   123                                                                        │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "ValueError: need at least one array to concatenate\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751126928/egodti7mls1x5qr83chy.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751126928/egodti7mls1x5qr83chy.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.40s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/upper_body/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/upper_body/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n",
            "[Keep-Alive] Preventing timeout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 16:22:22] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/upper_body/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/upper_body/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/upper_body/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Upper\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "Found 1 images in the folder /tmp/dresscode/upper_body\n",
            "saving custom FID stats to /usr/local/lib/python3.11/dist-packages/cleanfid/stats/dresscode_upper_body_clean_custom_na.npz\n",
            "saving custom KID stats to /usr/local/lib/python3.11/dist-packages/cleanfid/stats/dresscode_upper_body_clean_custom_na_kid.npz\n",
            "Found 0 images in the folder /tmp/dresscode/dresses\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751127747.647240   32470 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751127747.700048   32470 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'logit_scale', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.bias']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:18,  2.58it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:17,  2.77it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:16,  2.81it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:16,  2.84it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:15,  2.87it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:14,  2.89it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:14,  2.90it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:14,  2.89it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:13,  2.90it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:03<00:13,  2.89it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.90it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:12,  2.90it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:04<00:12,  2.89it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.88it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:11,  2.87it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:05<00:11,  2.88it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:11,  2.89it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:06<00:10,  2.88it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:06<00:10,  2.88it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.87it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:07<00:09,  2.87it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:09,  2.88it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.88it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:08<00:08,  2.88it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.85it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.81it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:09<00:07,  2.79it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.77it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:10<00:07,  2.77it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:10<00:06,  2.77it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.77it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:11<00:06,  2.79it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:11<00:05,  2.76it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.73it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:12<00:05,  2.74it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.78it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:13<00:04,  2.79it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:13<00:03,  2.81it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.83it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:14<00:03,  2.84it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:14<00:02,  2.84it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.84it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:15<00:02,  2.85it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:15<00:01,  2.84it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.85it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:16<00:01,  2.85it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:16<00:00,  2.84it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:17<00:00,  2.84it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.84it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.84it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:24<00:00, 24.15s/it]\n",
            "100%|██████████| 1/1 [00:24<00:00, 24.15s/it]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1123.88it/s]\n",
            "\n",
            "custom stats: upper_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "custom stats: upper_body : 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
            "custom stats: upper_body : 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]\n",
            "\n",
            "0it [00:00, ?it/s]\n",
            "0it [00:00, ?it/s]\n",
            "\n",
            "custom stats: dresses : : 0it [00:00, ?it/s]\n",
            "custom stats: dresses : : 0it [00:00, ?it/s]\n",
            "╭───────────────────── Traceback (most recent call last) ──────────────────────╮\n",
            "│ /content/ladi-vton/src/inference.py:347 in <module>                          │\n",
            "│                                                                              │\n",
            "│   344                                                                        │\n",
            "│   345                                                                        │\n",
            "│   346 if __name__ == \"__main__\":                                             │\n",
            "│ ❱ 347 │   main()                                                             │\n",
            "│   348                                                                        │\n",
            "│                                                                              │\n",
            "│ /usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:115 in    │\n",
            "│ decorate_context                                                             │\n",
            "│                                                                              │\n",
            "│   112 │   @functools.wraps(func)                                             │\n",
            "│   113 │   def decorate_context(*args, **kwargs):                             │\n",
            "│   114 │   │   with ctx_factory():                                            │\n",
            "│ ❱ 115 │   │   │   return func(*args, **kwargs)                               │\n",
            "│   116 │                                                                      │\n",
            "│   117 │   return decorate_context                                            │\n",
            "│   118                                                                        │\n",
            "│                                                                              │\n",
            "│ /content/ladi-vton/src/inference.py:339 in main                              │\n",
            "│                                                                              │\n",
            "│   336 │                                                                      │\n",
            "│   337 │   # Compute metrics if requested                                     │\n",
            "│   338 │   if args.compute_metrics:                                           │\n",
            "│ ❱ 339 │   │   metrics = compute_metrics(save_dir, args.test_order, args.data │\n",
            "│   340 │   │   │   │   │   │   │   │     args.dresscode_dataroot, args.vitonh │\n",
            "│   341 │   │                                                                  │\n",
            "│   342 │   │   with open(os.path.join(save_dir, f\"metrics_{args.test_order}_{ │\n",
            "│                                                                              │\n",
            "│ /content/ladi-vton/src/utils/val_metrics.py:156 in compute_metrics           │\n",
            "│                                                                              │\n",
            "│   153 │   │   │                                                              │\n",
            "│   154 │   │   │   # Check if FID stats exist, if not compute them            │\n",
            "│   155 │   │   │   if not fid.test_stats_exists(f\"{dataset}_{category}\", mode │\n",
            "│ ❱ 156 │   │   │   │   make_custom_stats(dresscode_dataroot, vitonhd_dataroot │\n",
            "│   157 │   │   │                                                              │\n",
            "│   158 │   │   │   # Compute FID score                                        │\n",
            "│   159 │   │   │   fid_score = fid.compute_fid(os.path.join(gen_folder, categ │\n",
            "│                                                                              │\n",
            "│ /content/ladi-vton/src/utils/generate_fid_stats.py:22 in make_custom_stats   │\n",
            "│                                                                              │\n",
            "│   19 │   │   │   │   os.makedirs(tmp_folder, exist_ok=True)                  │\n",
            "│   20 │   │   │   │   for path in tqdm(paths):                                │\n",
            "│   21 │   │   │   │   │   shutil.copy(path, tmp_folder)                       │\n",
            "│ ❱ 22 │   │   │   │   fid.make_custom_stats(f\"dresscode_{category}\", tmp_fold │\n",
            "│   23 │   │                                                                   │\n",
            "│   24 │   │   if not fid.test_stats_exists(f\"dresscode_all\", mode='clean'):   │\n",
            "│   25 │   │   │   paths = [os.path.join(dresscode_dataroot, category, 'images │\n",
            "│                                                                              │\n",
            "│ /usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:363 in               │\n",
            "│ make_custom_stats                                                            │\n",
            "│                                                                              │\n",
            "│   360 │   │   raise ValueError(f\"The entered model name - {model_name} was n │\n",
            "│   361 │                                                                      │\n",
            "│   362 │   # get all inception features for folder images                     │\n",
            "│ ❱ 363 │   np_feats = get_folder_features(fdir, feat_model, num_workers=num_w │\n",
            "│   364 │   │   │   │   │   │   │   │   │   batch_size=batch_size, device=devi │\n",
            "│   365 │   │   │   │   │   │   │   │   │   mode=mode, description=f\"custom st │\n",
            "│   366 │   │   │   │   │   │   │   │   │   custom_image_tranform=custom_image │\n",
            "│                                                                              │\n",
            "│ /usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:147 in               │\n",
            "│ get_folder_features                                                          │\n",
            "│                                                                              │\n",
            "│   144 │   │   │   random.seed(seed)                                          │\n",
            "│   145 │   │   │   random.shuffle(files)                                      │\n",
            "│   146 │   │   files = files[:num]                                            │\n",
            "│ ❱ 147 │   np_feats = get_files_features(files, model, num_workers=num_worker │\n",
            "│   148 │   │   │   │   │   │   │   │     batch_size=batch_size, device=device │\n",
            "│   149 │   │   │   │   │   │   │   │     custom_fn_resize=custom_fn_resize,   │\n",
            "│   150 │   │   │   │   │   │   │   │     custom_image_tranform=custom_image_t │\n",
            "│                                                                              │\n",
            "│ /usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:120 in               │\n",
            "│ get_files_features                                                           │\n",
            "│                                                                              │\n",
            "│   117 │                                                                      │\n",
            "│   118 │   for batch in pbar:                                                 │\n",
            "│   119 │   │   l_feats.append(get_batch_features(batch, model, device))       │\n",
            "│ ❱ 120 │   np_feats = np.concatenate(l_feats)                                 │\n",
            "│   121 │   return np_feats                                                    │\n",
            "│   122                                                                        │\n",
            "│   123                                                                        │\n",
            "╰──────────────────────────────────────────────────────────────────────────────╯\n",
            "ValueError: need at least one array to concatenate\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751127845/y2flmel8ajfcv2do6zuz.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751127845/y2flmel8ajfcv2do6zuz.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/upper_body/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/upper_body/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 16:25:59] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/upper_body/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/upper_body/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/upper_body/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Upper\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "compute FID of a folder with dresscode_upper_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/upper_body\n",
            "compute KID of a folder with dresscode_upper_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/upper_body\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751127964.423362   33441 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751127964.431331   33441 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'logit_scale', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:19,  2.49it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:17,  2.71it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:17,  2.76it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:16,  2.76it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:16,  2.79it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:15,  2.80it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:15,  2.80it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:14,  2.82it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:14,  2.83it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:14,  2.83it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:03<00:13,  2.84it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.83it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:13,  2.83it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:04<00:12,  2.83it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.83it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:12,  2.77it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:06<00:11,  2.76it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:11,  2.75it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:06<00:11,  2.71it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:10,  2.74it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.74it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:07<00:10,  2.72it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:09,  2.72it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.69it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:09<00:09,  2.68it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.73it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.74it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:10<00:07,  2.77it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.78it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:10<00:07,  2.79it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:06,  2.79it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.80it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:11<00:06,  2.79it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:05,  2.80it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.80it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:12<00:05,  2.79it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.80it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:13<00:04,  2.80it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:14<00:03,  2.80it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.80it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:14<00:03,  2.80it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:15<00:02,  2.80it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.79it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:15<00:02,  2.79it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:16<00:01,  2.77it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.77it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:16<00:01,  2.79it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:17<00:00,  2.78it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:17<00:00,  2.78it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.78it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.78it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.43s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.43s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "FID upper_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "FID upper_body : 100%|██████████| 1/1 [00:02<00:00,  2.41s/it]\n",
            "FID upper_body : 100%|██████████| 1/1 [00:03<00:00,  3.67s/it]\n",
            "\n",
            "KID upper_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "KID upper_body : 100%|██████████| 1/1 [00:01<00:00,  1.43s/it]\n",
            "KID upper_body : 100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:78: RuntimeWarning: invalid value encountered in divide\n",
            "  t += (a.sum() - np.diag(a).sum()) / (m - 1) - b.sum() * 2 / m\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "Downloading: \"https://github.com/toshas/torch-fidelity/releases/download/v0.2.0/weights-inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/weights-inception-2015-12-05-6726825d.pth\n",
            "\n",
            "  0%|          | 0.00/91.2M [00:00<?, ?B/s]\n",
            " 24%|██▎       | 21.5M/91.2M [00:00<00:00, 225MB/s]\n",
            " 58%|█████▊    | 52.8M/91.2M [00:00<00:00, 286MB/s]\n",
            " 90%|████████▉ | 82.1M/91.2M [00:00<00:00, 295MB/s]\n",
            "100%|██████████| 91.2M/91.2M [00:00<00:00, 291MB/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "\n",
            "  0%|          | 0.00/233M [00:00<?, ?B/s]\n",
            "  6%|▌         | 14.5M/233M [00:00<00:02, 111MB/s]\n",
            " 11%|█         | 25.1M/233M [00:05<00:51, 4.23MB/s]\n",
            " 14%|█▍        | 32.4M/233M [00:05<00:33, 6.20MB/s]\n",
            " 22%|██▏       | 50.1M/233M [00:05<00:14, 12.9MB/s]\n",
            " 31%|███▏      | 73.2M/233M [00:05<00:06, 24.5MB/s]\n",
            " 38%|███▊      | 88.3M/233M [00:05<00:05, 30.2MB/s]\n",
            " 47%|████▋     | 109M/233M [00:05<00:02, 44.9MB/s] \n",
            " 53%|█████▎    | 123M/233M [00:06<00:03, 35.6MB/s]\n",
            " 57%|█████▋    | 134M/233M [00:06<00:02, 34.8MB/s]\n",
            " 61%|██████    | 142M/233M [00:09<00:07, 12.6MB/s]\n",
            " 65%|██████▌   | 152M/233M [00:09<00:05, 16.3MB/s]\n",
            " 77%|███████▋  | 179M/233M [00:09<00:01, 30.8MB/s]\n",
            " 84%|████████▎ | 195M/233M [00:09<00:01, 38.4MB/s]\n",
            " 97%|█████████▋| 227M/233M [00:09<00:00, 64.6MB/s]\n",
            "100%|██████████| 233M/233M [00:09<00:00, 25.0MB/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.55it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.28it/s]\n",
            "\n",
            "[Keep-Alive] Preventing timeout...\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751128091/dypbtvkrdrw7ms0rfwfk.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751128091/dypbtvkrdrw7ms0rfwfk.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/lower_body/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/lower_body/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 16:32:58] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/lower_body/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/lower_body/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/lower_body/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Lower\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "compute FID of a folder with dresscode_lower_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/lower_body\n",
            "compute KID of a folder with dresscode_lower_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/lower_body\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751128383.639746   35393 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751128383.693165   35393 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_projection.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'logit_scale', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:19,  2.51it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:17,  2.74it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:16,  2.79it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:16,  2.73it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:16,  2.75it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:16,  2.75it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:16,  2.65it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:15,  2.68it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:15,  2.71it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:14,  2.72it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:04<00:14,  2.74it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.73it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:13,  2.72it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:05<00:13,  2.72it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.75it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:12,  2.79it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:06<00:11,  2.81it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:11,  2.81it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:06<00:10,  2.83it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:10,  2.83it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.83it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:07<00:09,  2.83it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:09,  2.83it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.83it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:09<00:08,  2.83it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.84it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.83it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:10<00:07,  2.83it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.82it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:10<00:07,  2.81it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:06,  2.83it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.81it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:11<00:06,  2.83it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:05,  2.82it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.81it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:12<00:04,  2.82it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.83it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:13<00:04,  2.81it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:13<00:03,  2.81it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.82it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:14<00:03,  2.80it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:15<00:02,  2.79it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.76it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:15<00:02,  2.74it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:16<00:01,  2.73it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.72it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:16<00:01,  2.72it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:17<00:00,  2.72it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:17<00:00,  2.73it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.72it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.77it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.32s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.32s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "FID lower_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "FID lower_body : 100%|██████████| 1/1 [00:01<00:00,  1.64s/it]\n",
            "FID lower_body : 100%|██████████| 1/1 [00:02<00:00,  2.58s/it]\n",
            "\n",
            "KID lower_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "KID lower_body : 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
            "KID lower_body : 100%|██████████| 1/1 [00:02<00:00,  2.31s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:78: RuntimeWarning: invalid value encountered in divide\n",
            "  t += (a.sum() - np.diag(a).sum()) / (m - 1) - b.sum() * 2 / m\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.89it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751128501/fv83e2wkcpdnmb73ptut.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751128501/fv83e2wkcpdnmb73ptut.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.55s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/dresses/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/dresses/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 16:38:09] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/dresses/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/dresses/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/dresses/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Full\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "Found 1 images in the folder /tmp/dresscode/dresses\n",
            "saving custom FID stats to /usr/local/lib/python3.11/dist-packages/cleanfid/stats/dresscode_dresses_clean_custom_na.npz\n",
            "saving custom KID stats to /usr/local/lib/python3.11/dist-packages/cleanfid/stats/dresscode_dresses_clean_custom_na_kid.npz\n",
            "Found 1 images in the folder /tmp/dresscode/all\n",
            "saving custom FID stats to /usr/local/lib/python3.11/dist-packages/cleanfid/stats/dresscode_all_clean_custom_na.npz\n",
            "saving custom KID stats to /usr/local/lib/python3.11/dist-packages/cleanfid/stats/dresscode_all_clean_custom_na_kid.npz\n",
            "compute FID of a folder with dresscode_dresses statistics\n",
            "Found 1 images in the folder /content/output/unpaired/dresses\n",
            "compute KID of a folder with dresscode_dresses statistics\n",
            "Found 1 images in the folder /content/output/unpaired/dresses\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751128694.576986   36860 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751128694.589586   36860 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'logit_scale', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_projection.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:19,  2.55it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:17,  2.75it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:16,  2.78it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:16,  2.81it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:15,  2.83it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:15,  2.83it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:15,  2.83it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:14,  2.81it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:14,  2.84it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:14,  2.83it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:03<00:13,  2.83it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.84it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:13,  2.84it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:04<00:12,  2.85it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.84it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:11,  2.84it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:06<00:11,  2.85it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:11,  2.84it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:06<00:10,  2.84it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:10,  2.83it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.84it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:07<00:09,  2.82it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:09,  2.81it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.76it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:08<00:09,  2.76it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.75it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.73it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:09<00:08,  2.73it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.72it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:10<00:07,  2.72it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:07,  2.71it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.69it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:11<00:06,  2.69it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:05,  2.71it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.73it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:12<00:05,  2.75it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.75it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:13<00:04,  2.76it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:14<00:03,  2.78it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.77it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:14<00:03,  2.78it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:15<00:02,  2.79it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.79it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:15<00:02,  2.79it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:16<00:01,  2.79it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.80it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:16<00:01,  2.79it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:17<00:00,  2.79it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:17<00:00,  2.79it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.80it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.79it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.36s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.36s/it]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 671.20it/s]\n",
            "\n",
            "custom stats: dresses :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "custom stats: dresses : 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n",
            "custom stats: dresses : 100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 4080.06it/s]\n",
            "\n",
            "custom stats: all :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "custom stats: all : 100%|██████████| 1/1 [00:00<00:00,  7.85it/s]\n",
            "custom stats: all : 100%|██████████| 1/1 [00:00<00:00,  7.83it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "FID dresses :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "FID dresses : 100%|██████████| 1/1 [00:01<00:00,  1.48s/it]\n",
            "FID dresses : 100%|██████████| 1/1 [00:02<00:00,  2.92s/it]\n",
            "\n",
            "KID dresses :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "KID dresses : 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
            "KID dresses : 100%|██████████| 1/1 [00:03<00:00,  3.12s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:78: RuntimeWarning: invalid value encountered in divide\n",
            "  t += (a.sum() - np.diag(a).sum()) / (m - 1) - b.sum() * 2 / m\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.42it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751128814/xjgy5duuxokmtkle4s6z.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751128814/xjgy5duuxokmtkle4s6z.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.56s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/dresses/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/dresses/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n",
            "[Keep-Alive] Preventing timeout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 16:48:23] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/dresses/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/dresses/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/dresses/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Full\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "compute FID of a folder with dresscode_dresses statistics\n",
            "Found 1 images in the folder /content/output/unpaired/dresses\n",
            "compute KID of a folder with dresscode_dresses statistics\n",
            "Found 1 images in the folder /content/output/unpaired/dresses\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751129308.759346   39581 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751129308.769371   39581 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'logit_scale', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_projection.weight', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.final_layer_norm.weight']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:19,  2.56it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:17,  2.77it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:16,  2.80it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:16,  2.85it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:15,  2.86it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:15,  2.86it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:14,  2.87it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:14,  2.86it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:13,  2.86it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:03<00:13,  2.86it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.87it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:12,  2.88it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:04<00:12,  2.87it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.87it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:11,  2.84it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:05<00:11,  2.80it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:11,  2.80it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:06<00:11,  2.77it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:10,  2.75it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.77it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:07<00:10,  2.77it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:09,  2.75it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.72it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:08<00:09,  2.71it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.73it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.76it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:09<00:07,  2.79it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.79it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:10<00:07,  2.80it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:06,  2.83it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.83it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:11<00:06,  2.83it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:05,  2.84it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.84it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:12<00:04,  2.84it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.84it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:13<00:04,  2.85it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:13<00:03,  2.84it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.84it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:14<00:03,  2.85it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:14<00:02,  2.84it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.84it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:15<00:02,  2.83it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:15<00:01,  2.83it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.84it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:16<00:01,  2.83it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:17<00:00,  2.83it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:17<00:00,  2.83it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.82it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.82it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.12s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.12s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "FID dresses :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "FID dresses : 100%|██████████| 1/1 [00:02<00:00,  2.40s/it]\n",
            "FID dresses : 100%|██████████| 1/1 [00:03<00:00,  3.82s/it]\n",
            "\n",
            "KID dresses :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "KID dresses : 100%|██████████| 1/1 [00:01<00:00,  1.41s/it]\n",
            "KID dresses : 100%|██████████| 1/1 [00:02<00:00,  2.30s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:78: RuntimeWarning: invalid value encountered in divide\n",
            "  t += (a.sum() - np.diag(a).sum()) / (m - 1) - b.sum() * 2 / m\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.59it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751129427/fwxhvbdiqxp72powl2jz.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751129427/fwxhvbdiqxp72powl2jz.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.54s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/dresses/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/dresses/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 16:53:02] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/dresses/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/dresses/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/dresses/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Full\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "compute FID of a folder with dresscode_dresses statistics\n",
            "Found 1 images in the folder /content/output/unpaired/dresses\n",
            "compute KID of a folder with dresscode_dresses statistics\n",
            "Found 1 images in the folder /content/output/unpaired/dresses\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751129587.935212   40920 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751129587.944055   40920 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'logit_scale', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_projection.weight']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:19,  2.51it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:17,  2.73it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:16,  2.78it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:16,  2.82it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:15,  2.86it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:15,  2.87it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:14,  2.87it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:14,  2.88it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:14,  2.87it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:13,  2.87it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:03<00:13,  2.87it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.86it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:12,  2.86it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:04<00:12,  2.86it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.86it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:11,  2.86it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:05<00:11,  2.86it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:11,  2.85it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:06<00:10,  2.85it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:10,  2.86it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.83it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:07<00:09,  2.80it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:09,  2.80it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.75it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:08<00:09,  2.74it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.74it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.71it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:09<00:08,  2.72it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.69it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:10<00:07,  2.70it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:07,  2.69it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.72it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:11<00:06,  2.75it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:05,  2.79it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.78it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:12<00:04,  2.81it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.82it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:13<00:04,  2.82it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:13<00:03,  2.84it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.84it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:14<00:03,  2.83it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:14<00:02,  2.84it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.84it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:15<00:02,  2.84it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:16<00:01,  2.84it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.85it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:16<00:01,  2.84it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:17<00:00,  2.84it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:17<00:00,  2.83it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.83it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.81it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.29s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.29s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "FID dresses :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "FID dresses : 100%|██████████| 1/1 [00:01<00:00,  1.72s/it]\n",
            "FID dresses : 100%|██████████| 1/1 [00:03<00:00,  3.19s/it]\n",
            "\n",
            "KID dresses :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "KID dresses : 100%|██████████| 1/1 [00:02<00:00,  2.10s/it]\n",
            "KID dresses : 100%|██████████| 1/1 [00:03<00:00,  3.05s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:78: RuntimeWarning: invalid value encountered in divide\n",
            "  t += (a.sum() - np.diag(a).sum()) / (m - 1) - b.sum() * 2 / m\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.28it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751129706/y4hcc1z6k2dfcnsjolu5.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751129706/y4hcc1z6k2dfcnsjolu5.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/lower_body/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/lower_body/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n",
            "[Keep-Alive] Preventing timeout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 16:57:12] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/lower_body/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/lower_body/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/lower_body/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Lower\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "compute FID of a folder with dresscode_lower_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/lower_body\n",
            "compute KID of a folder with dresscode_lower_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/lower_body\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751129837.424198   42143 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751129837.432661   42143 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'logit_scale', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_projection.weight', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.bias']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:19,  2.45it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:18,  2.63it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:17,  2.70it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:16,  2.73it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:16,  2.75it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:15,  2.76it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:15,  2.77it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:15,  2.77it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:14,  2.77it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:14,  2.77it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:04<00:14,  2.77it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.77it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:13,  2.75it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:05<00:13,  2.72it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.71it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:12,  2.68it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:06<00:12,  2.65it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:12,  2.67it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:07<00:11,  2.66it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:11,  2.65it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.66it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:08<00:10,  2.63it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:10,  2.65it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.68it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:09<00:09,  2.69it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.72it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.73it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:10<00:08,  2.72it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.73it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:11<00:07,  2.74it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:06,  2.72it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.73it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:12<00:06,  2.71it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:05,  2.71it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.72it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:13<00:05,  2.73it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.73it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:14<00:04,  2.71it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:14<00:04,  2.70it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.71it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:15<00:03,  2.72it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:15<00:02,  2.71it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.71it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:16<00:02,  2.71it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:16<00:01,  2.71it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.71it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:17<00:01,  2.71it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:17<00:00,  2.70it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:18<00:00,  2.70it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.70it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.71it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.76s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.76s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "FID lower_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "FID lower_body : 100%|██████████| 1/1 [00:02<00:00,  2.18s/it]\n",
            "FID lower_body : 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\n",
            "\n",
            "KID lower_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "KID lower_body : 100%|██████████| 1/1 [00:01<00:00,  1.44s/it]\n",
            "KID lower_body : 100%|██████████| 1/1 [00:02<00:00,  2.37s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:78: RuntimeWarning: invalid value encountered in divide\n",
            "  t += (a.sum() - np.diag(a).sum()) / (m - 1) - b.sum() * 2 / m\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  6.65it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751129954/pjo9v7ysix5b6fhfl9du.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751129954/pjo9v7ysix5b6fhfl9du.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.62s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/upper_body/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/upper_body/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 17:02:43] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/upper_body/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/upper_body/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/upper_body/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Upper\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "compute FID of a folder with dresscode_upper_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/upper_body\n",
            "compute KID of a folder with dresscode_upper_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/upper_body\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751130168.279015   43693 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751130168.287744   43693 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.19.layer_norm2.bias', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'logit_scale', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_projection.weight', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.k_proj.bias']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:20,  2.44it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:18,  2.61it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:17,  2.65it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:17,  2.68it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:16,  2.70it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:16,  2.70it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:15,  2.70it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:15,  2.71it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:15,  2.71it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:14,  2.70it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:04<00:14,  2.71it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:14,  2.70it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:13,  2.69it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:05<00:13,  2.62it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:13,  2.61it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:06<00:13,  2.61it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:06<00:12,  2.60it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:12,  2.60it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:07<00:11,  2.61it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:11,  2.61it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:11,  2.56it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:08<00:10,  2.55it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:10,  2.57it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:09<00:09,  2.60it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:09<00:09,  2.63it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:09,  2.63it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:10<00:08,  2.66it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:10<00:08,  2.65it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.64it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:07,  2.65it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:12<00:06,  2.66it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:12<00:06,  2.66it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:06,  2.65it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:13<00:05,  2.66it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:13<00:05,  2.67it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.66it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:14<00:04,  2.66it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:14<00:04,  2.68it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:15<00:03,  2.67it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:15<00:03,  2.67it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:15<00:03,  2.66it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:16<00:02,  2.65it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:16<00:02,  2.64it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:17<00:01,  2.64it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:17<00:01,  2.64it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:17<00:01,  2.65it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:18<00:00,  2.63it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:18<00:00,  2.63it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.60it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:18<00:00,  2.64it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:26<00:00, 26.49s/it]\n",
            "100%|██████████| 1/1 [00:26<00:00, 26.49s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "FID upper_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "FID upper_body : 100%|██████████| 1/1 [00:02<00:00,  2.15s/it]\n",
            "FID upper_body : 100%|██████████| 1/1 [00:03<00:00,  3.20s/it]\n",
            "\n",
            "KID upper_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "KID upper_body : 100%|██████████| 1/1 [00:01<00:00,  1.45s/it]\n",
            "KID upper_body : 100%|██████████| 1/1 [00:02<00:00,  2.48s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:78: RuntimeWarning: invalid value encountered in divide\n",
            "  t += (a.sum() - np.diag(a).sum()) / (m - 1) - b.sum() * 2 / m\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  7.16it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  1.02it/s]\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751130289/n4pcfawo0ujlc63t9zrk.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751130289/n4pcfawo0ujlc63t9zrk.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "\n",
            "=== New Request Received ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/google/cloud/firestore_v1/base_collection.py:304: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  return query.where(field_path, op_string, value)\n",
            "/tmp/ipython-input-20-823119022.py:16: UserWarning: Detected filter using positional arguments. Prefer using the 'filter' keyword argument instead.\n",
            "  query = db.collection(\"TryOn\").where(\"user_id\", \"==\", user_id).where(\"status\", \"==\", \"pending\").stream()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Updated request status to 'processing'.\n",
            "✅ Successfully downloaded images.\n",
            "🔍 Parsing image...\n",
            "----- STDOUT -----\n",
            " Evaluating total class number 18 with ['Background', 'Hat', 'Hair', 'Sunglasses', 'Upper-clothes', 'Skirt', 'Pants', 'Dress', 'Belt', 'Left-shoe', 'Right-shoe', 'Face', 'Left-leg', 'Right-leg', 'Left-arm', 'Right-arm', 'Bag', 'Scarf']\n",
            "\n",
            "----- STDERR -----\n",
            " /usr/local/lib/python3.8/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "simple_extractor.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(args.model_restore)['state_dict']\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.81s/it]\n",
            "\n",
            "⚙ Initializing...\n",
            "Processing image: input_images/lower_body/images/1111_1.jpg\n",
            "✅ Saved mask to: input_images/lower_body/masks/1111_1.png\n",
            "🔁 Environment already set up. Skipping setup.\n",
            "📷 Saved input image to: /content/pytorch-openpose/inputs/1111_0.jpg\n",
            "🔍 Running keypoint detection...\n",
            "[Keep-Alive] Preventing timeout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [28/Jun/2025 17:12:12] \"\u001b[35m\u001b[1mPOST /run_all HTTP/1.1\u001b[0m\" 202 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Inference successful.\n",
            "Saved: /content/pytorch-openpose/skeleton_results/1111_5.png, /content/pytorch-openpose/keypoints_results/1111_2.json\n",
            "\n",
            "📄 Saved keypoints JSON to: input_images/lower_body/keypoints/1111_2.json\n",
            "🖼 Saved skeleton image to: input_images/lower_body/skeletons/1111_5.png\n",
            "file_Nammmmmmme 1111_0.png\n",
            "file_Nammmmmmme_paaaathhhhh /content/output/unpaired/lower_body/1111_0.png\n",
            "Categooooooooryyyyyyyyyy_Nammmmmmmmmmmmmmmmmmmmmmme Lower\n",
            "🔁 Background: Starting inference...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n",
            "✅ Inference stdout:\n",
            "initialization method [normal]\n",
            "initialization method [normal]\n",
            "compute FID of a folder with dresscode_lower_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/lower_body\n",
            "compute KID of a folder with dresscode_lower_body statistics\n",
            "Found 1 images in the folder /content/output/unpaired/lower_body\n",
            "\n",
            "⚠️ Inference stderr (non-fatal):\n",
            "Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1751130737.771911   46291 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1751130737.821352   46291 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Some weights of the model checkpoint at laion/CLIP-ViT-H-14-laion2B-s32B-b79K were not used when initializing CLIPVisionModelWithProjection: ['text_model.encoder.layers.13.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm2.bias', 'text_model.encoder.layers.20.self_attn.out_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.bias', 'text_model.encoder.layers.22.self_attn.q_proj.bias', 'text_model.encoder.layers.12.self_attn.v_proj.bias', 'text_model.encoder.layers.10.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.bias', 'text_model.final_layer_norm.bias', 'text_model.encoder.layers.3.self_attn.v_proj.bias', 'text_model.encoder.layers.1.self_attn.q_proj.bias', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.20.mlp.fc1.bias', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.16.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.k_proj.bias', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.15.self_attn.v_proj.bias', 'text_model.encoder.layers.20.layer_norm1.weight', 'text_model.encoder.layers.3.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.embeddings.position_embedding.weight', 'text_model.encoder.layers.16.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.bias', 'text_model.encoder.layers.4.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.13.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.bias', 'text_model.encoder.layers.12.self_attn.q_proj.bias', 'text_model.encoder.layers.23.layer_norm1.weight', 'text_model.encoder.layers.15.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.k_proj.bias', 'text_model.encoder.layers.23.layer_norm2.weight', 'text_model.encoder.layers.22.self_attn.out_proj.weight', 'text_model.encoder.layers.17.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.bias', 'text_model.encoder.layers.22.mlp.fc2.weight', 'text_model.encoder.layers.18.layer_norm2.bias', 'text_model.encoder.layers.18.self_attn.q_proj.bias', 'text_model.encoder.layers.21.self_attn.v_proj.bias', 'text_model.encoder.layers.3.layer_norm2.weight', 'text_model.encoder.layers.10.self_attn.q_proj.bias', 'text_model.encoder.layers.11.layer_norm1.weight', 'text_model.encoder.layers.8.self_attn.v_proj.bias', 'text_model.encoder.layers.23.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.out_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.bias', 'text_model.encoder.layers.5.layer_norm1.weight', 'text_model.encoder.layers.17.self_attn.k_proj.bias', 'text_model.encoder.layers.1.layer_norm2.bias', 'text_model.encoder.layers.1.layer_norm2.weight', 'text_model.encoder.layers.13.mlp.fc2.weight', 'text_model.encoder.layers.19.self_attn.k_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc1.bias', 'text_model.encoder.layers.15.self_attn.q_proj.weight', 'text_model.encoder.layers.8.layer_norm1.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.bias', 'text_model.encoder.layers.17.self_attn.v_proj.weight', 'text_model.embeddings.token_embedding.weight', 'text_model.encoder.layers.2.mlp.fc2.bias', 'text_model.encoder.layers.21.layer_norm1.bias', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.v_proj.bias', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.19.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.bias', 'text_model.encoder.layers.22.layer_norm1.bias', 'text_model.encoder.layers.1.self_attn.v_proj.bias', 'text_model.encoder.layers.14.mlp.fc1.bias', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.23.self_attn.q_proj.weight', 'text_model.encoder.layers.13.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.14.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.bias', 'text_model.encoder.layers.8.self_attn.q_proj.bias', 'text_model.encoder.layers.20.self_attn.v_proj.weight', 'text_model.encoder.layers.10.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.v_proj.weight', 'text_model.encoder.layers.12.self_attn.q_proj.weight', 'text_model.encoder.layers.20.mlp.fc2.weight', 'text_model.encoder.layers.22.mlp.fc1.bias', 'text_model.encoder.layers.3.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm1.bias', 'text_model.encoder.layers.7.layer_norm1.weight', 'text_model.encoder.layers.20.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.9.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.v_proj.bias', 'text_model.encoder.layers.19.self_attn.k_proj.weight', 'text_model.encoder.layers.20.layer_norm1.bias', 'text_model.encoder.layers.22.layer_norm1.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.bias', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.bias', 'text_model.encoder.layers.19.self_attn.v_proj.weight', 'text_model.encoder.layers.13.self_attn.v_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.18.layer_norm1.bias', 'text_model.encoder.layers.12.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.5.layer_norm2.bias', 'text_model.encoder.layers.13.self_attn.k_proj.weight', 'text_model.encoder.layers.16.self_attn.v_proj.bias', 'text_model.encoder.layers.3.self_attn.k_proj.bias', 'text_model.encoder.layers.15.self_attn.q_proj.bias', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.k_proj.bias', 'text_model.encoder.layers.6.layer_norm2.bias', 'text_model.encoder.layers.14.mlp.fc1.weight', 'text_model.encoder.layers.13.self_attn.out_proj.bias', 'text_model.encoder.layers.2.self_attn.k_proj.bias', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.2.layer_norm2.bias', 'text_model.encoder.layers.6.self_attn.k_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.bias', 'text_model.encoder.layers.1.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.0.layer_norm1.bias', 'text_model.encoder.layers.13.layer_norm2.bias', 'text_model.encoder.layers.0.mlp.fc2.bias', 'text_model.encoder.layers.6.mlp.fc2.bias', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.15.self_attn.k_proj.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layers.13.layer_norm1.weight', 'text_model.encoder.layers.5.self_attn.out_proj.bias', 'text_model.encoder.layers.23.self_attn.k_proj.weight', 'text_model.final_layer_norm.weight', 'text_model.encoder.layers.14.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.v_proj.bias', 'text_model.encoder.layers.9.self_attn.v_proj.bias', 'text_model.encoder.layers.20.layer_norm2.bias', 'text_model.encoder.layers.21.self_attn.q_proj.bias', 'text_model.encoder.layers.6.self_attn.q_proj.bias', 'text_model.encoder.layers.14.self_attn.k_proj.bias', 'text_model.encoder.layers.22.mlp.fc1.weight', 'text_model.encoder.layers.17.self_attn.k_proj.weight', 'text_model.encoder.layers.15.mlp.fc2.weight', 'text_model.encoder.layers.23.mlp.fc1.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.bias', 'text_model.encoder.layers.18.self_attn.q_proj.weight', 'text_model.encoder.layers.15.layer_norm2.bias', 'text_model.encoder.layers.17.mlp.fc1.bias', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.13.mlp.fc2.bias', 'text_model.encoder.layers.21.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'text_model.encoder.layers.23.mlp.fc2.bias', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.18.mlp.fc2.weight', 'text_model.encoder.layers.21.self_attn.k_proj.bias', 'text_model.encoder.layers.12.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.bias', 'text_model.encoder.layers.19.mlp.fc1.bias', 'text_model.encoder.layers.22.layer_norm2.bias', 'text_model.encoder.layers.22.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.bias', 'text_model.encoder.layers.10.layer_norm1.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.14.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.k_proj.bias', 'text_model.encoder.layers.17.self_attn.q_proj.bias', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.20.self_attn.k_proj.bias', 'text_model.encoder.layers.15.mlp.fc2.bias', 'text_model.encoder.layers.15.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.q_proj.bias', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.0.mlp.fc1.bias', 'text_model.encoder.layers.0.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.bias', 'text_model.encoder.layers.12.mlp.fc1.weight', 'text_model.encoder.layers.6.self_attn.out_proj.bias', 'text_model.encoder.layers.21.mlp.fc1.weight', 'text_model.encoder.layers.3.self_attn.out_proj.bias', 'text_model.encoder.layers.3.layer_norm1.weight', 'text_model.encoder.layers.17.mlp.fc2.bias', 'text_model.encoder.layers.21.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.23.self_attn.out_proj.bias', 'text_model.encoder.layers.11.self_attn.q_proj.bias', 'text_model.encoder.layers.17.layer_norm1.weight', 'text_model.encoder.layers.13.self_attn.out_proj.weight', 'text_model.encoder.layers.23.mlp.fc1.bias', 'text_model.encoder.layers.21.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.0.layer_norm1.weight', 'text_model.encoder.layers.0.self_attn.q_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.16.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.19.layer_norm2.bias', 'logit_scale', 'text_model.encoder.layers.14.layer_norm2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.bias', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.12.self_attn.v_proj.weight', 'text_model.encoder.layers.21.self_attn.out_proj.bias', 'text_model.encoder.layers.9.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.v_proj.bias', 'text_model.encoder.layers.1.mlp.fc1.bias', 'text_model.encoder.layers.17.layer_norm2.bias', 'text_model.encoder.layers.14.self_attn.k_proj.weight', 'text_model.encoder.layers.9.layer_norm1.weight', 'text_model.encoder.layers.3.self_attn.q_proj.bias', 'text_model.encoder.layers.12.mlp.fc2.weight', 'text_model.encoder.layers.18.layer_norm2.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.12.self_attn.out_proj.bias', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.19.self_attn.out_proj.weight', 'text_model.encoder.layers.14.self_attn.q_proj.weight', 'text_model.encoder.layers.17.self_attn.out_proj.weight', 'text_model.encoder.layers.23.self_attn.out_proj.weight', 'text_model.encoder.layers.13.self_attn.k_proj.bias', 'text_model.encoder.layers.10.mlp.fc1.bias', 'text_model.encoder.layers.16.mlp.fc2.bias', 'text_model.encoder.layers.9.self_attn.k_proj.bias', 'text_model.encoder.layers.1.self_attn.out_proj.bias', 'text_model.encoder.layers.16.layer_norm2.weight', 'text_model.encoder.layers.16.mlp.fc2.weight', 'text_model.encoder.layers.14.mlp.fc2.bias', 'text_model.encoder.layers.15.layer_norm1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.14.layer_norm2.bias', 'text_model.encoder.layers.21.layer_norm2.bias', 'text_projection.weight', 'text_model.encoder.layers.6.layer_norm1.weight', 'text_model.encoder.layers.15.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm2.weight', 'text_model.encoder.layers.16.layer_norm1.bias', 'text_model.encoder.layers.5.layer_norm2.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.18.mlp.fc2.bias', 'text_model.encoder.layers.20.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.23.layer_norm1.bias', 'text_model.encoder.layers.7.mlp.fc1.bias', 'text_model.encoder.layers.23.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.bias', 'text_model.encoder.layers.8.layer_norm1.bias', 'text_model.encoder.layers.14.layer_norm1.bias', 'text_model.encoder.layers.4.layer_norm1.bias', 'text_model.encoder.layers.7.self_attn.out_proj.bias', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.1.layer_norm1.bias', 'text_model.encoder.layers.22.self_attn.v_proj.weight', 'text_model.encoder.layers.21.mlp.fc1.bias', 'text_model.encoder.layers.19.mlp.fc2.weight', 'text_model.encoder.layers.23.layer_norm2.bias', 'text_model.encoder.layers.15.self_attn.out_proj.weight', 'text_model.encoder.layers.14.layer_norm1.weight', 'text_model.encoder.layers.4.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm1.weight', 'text_model.encoder.layers.10.layer_norm2.bias', 'text_model.encoder.layers.5.layer_norm1.bias', 'text_model.encoder.layers.2.layer_norm1.bias', 'text_model.encoder.layers.19.self_attn.q_proj.bias', 'text_model.encoder.layers.22.mlp.fc2.bias', 'text_model.encoder.layers.7.layer_norm1.bias', 'text_model.encoder.layers.9.layer_norm2.bias', 'text_model.encoder.layers.2.mlp.fc1.bias', 'text_model.encoder.layers.13.self_attn.q_proj.bias', 'text_model.encoder.layers.18.self_attn.k_proj.bias', 'text_model.encoder.layers.16.self_attn.q_proj.bias', 'text_model.encoder.layers.22.self_attn.out_proj.bias', 'text_model.encoder.layers.16.self_attn.k_proj.bias', 'text_model.encoder.layers.0.self_attn.v_proj.bias', 'text_model.encoder.layers.16.self_attn.k_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.2.self_attn.v_proj.bias', 'text_model.encoder.layers.4.layer_norm2.bias', 'text_model.encoder.layers.11.self_attn.v_proj.bias', 'text_model.encoder.layers.15.self_attn.v_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.18.mlp.fc1.bias', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.8.layer_norm2.bias', 'text_model.encoder.layers.17.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm2.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.bias', 'text_model.encoder.layers.5.self_attn.k_proj.bias', 'text_model.encoder.layers.18.layer_norm1.weight', 'text_model.encoder.layers.11.self_attn.k_proj.bias', 'text_model.encoder.layers.2.self_attn.out_proj.bias', 'text_model.encoder.layers.10.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.out_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.bias', 'text_model.encoder.layers.21.layer_norm1.weight', 'text_model.encoder.layers.16.layer_norm2.bias', 'text_model.encoder.layers.21.layer_norm2.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.2.layer_norm1.weight', 'text_model.encoder.layers.15.self_attn.out_proj.bias', 'text_model.encoder.layers.16.layer_norm1.weight', 'text_model.encoder.layers.21.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.11.mlp.fc2.bias', 'text_model.encoder.layers.11.layer_norm1.bias', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.3.mlp.fc2.bias', 'text_model.encoder.layers.8.self_attn.out_proj.bias', 'text_model.encoder.layers.7.layer_norm2.weight', 'text_model.encoder.layers.11.self_attn.out_proj.bias', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.v_proj.bias', 'text_model.encoder.layers.16.mlp.fc1.weight', 'text_model.encoder.layers.13.mlp.fc1.weight', 'text_model.encoder.layers.19.self_attn.v_proj.bias', 'text_model.encoder.layers.17.self_attn.v_proj.bias', 'text_model.encoder.layers.14.self_attn.out_proj.weight', 'text_model.encoder.layers.18.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.v_proj.bias', 'text_model.encoder.layers.4.mlp.fc1.bias', 'text_model.encoder.layers.6.self_attn.v_proj.bias', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.mlp.fc2.bias', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.bias', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.bias', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.4.mlp.fc2.bias', 'text_model.encoder.layers.19.self_attn.out_proj.bias', 'text_model.encoder.layers.20.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.mlp.fc2.bias', 'text_model.encoder.layers.19.layer_norm2.weight', 'text_model.encoder.layers.19.mlp.fc2.bias', 'text_model.encoder.layers.20.self_attn.q_proj.weight', 'text_model.encoder.layers.7.layer_norm2.bias', 'text_model.encoder.layers.8.layer_norm2.weight', 'text_model.encoder.layers.21.mlp.fc2.bias', 'text_model.encoder.layers.18.self_attn.out_proj.bias', 'text_model.encoder.layers.15.mlp.fc1.weight', 'text_model.encoder.layers.12.layer_norm1.bias', 'text_model.encoder.layers.19.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.11.layer_norm2.weight', 'text_model.encoder.layers.23.self_attn.v_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.bias', 'text_model.encoder.layers.20.layer_norm2.weight', 'text_model.encoder.layers.6.layer_norm2.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.21.self_attn.q_proj.weight', 'text_model.encoder.layers.10.mlp.fc2.bias', 'text_model.encoder.layers.4.self_attn.k_proj.bias', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.19.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.12.mlp.fc2.bias', 'text_model.encoder.layers.17.mlp.fc2.weight', 'text_model.encoder.layers.6.layer_norm1.bias', 'text_model.encoder.layers.15.layer_norm2.weight', 'text_model.encoder.layers.17.self_attn.q_proj.weight', 'text_model.encoder.layers.20.self_attn.q_proj.bias', 'text_model.encoder.layers.17.mlp.fc1.weight', 'text_model.encoder.layers.18.mlp.fc1.weight', 'text_model.encoder.layers.13.self_attn.v_proj.weight', 'text_model.encoder.layers.22.layer_norm2.weight', 'text_model.encoder.layers.12.layer_norm2.bias', 'text_model.encoder.layers.17.self_attn.out_proj.bias', 'text_model.encoder.layers.13.layer_norm1.bias', 'text_model.encoder.layers.2.self_attn.k_proj.weight']\n",
            "- This IS expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing CLIPVisionModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Using cache found in /root/.cache/torch/hub/miccunifi_ladi-vton_master\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  warnings.warn(\n",
            "\n",
            "\n",
            "  0%|          | 0/50 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "  2%|▏         | 1/50 [00:00<00:21,  2.30it/s]\u001b[A\n",
            "\n",
            "  4%|▍         | 2/50 [00:00<00:18,  2.55it/s]\u001b[A\n",
            "\n",
            "  6%|▌         | 3/50 [00:01<00:17,  2.63it/s]\u001b[A\n",
            "\n",
            "  8%|▊         | 4/50 [00:01<00:17,  2.70it/s]\u001b[A\n",
            "\n",
            " 10%|█         | 5/50 [00:01<00:16,  2.74it/s]\u001b[A\n",
            "\n",
            " 12%|█▏        | 6/50 [00:02<00:16,  2.74it/s]\u001b[A\n",
            "\n",
            " 14%|█▍        | 7/50 [00:02<00:15,  2.73it/s]\u001b[A\n",
            "\n",
            " 16%|█▌        | 8/50 [00:02<00:15,  2.71it/s]\u001b[A\n",
            "\n",
            " 18%|█▊        | 9/50 [00:03<00:14,  2.76it/s]\u001b[A\n",
            "\n",
            " 20%|██        | 10/50 [00:03<00:14,  2.80it/s]\u001b[A\n",
            "\n",
            " 22%|██▏       | 11/50 [00:04<00:13,  2.81it/s]\u001b[A\n",
            "\n",
            " 24%|██▍       | 12/50 [00:04<00:13,  2.83it/s]\u001b[A\n",
            "\n",
            " 26%|██▌       | 13/50 [00:04<00:13,  2.84it/s]\u001b[A\n",
            "\n",
            " 28%|██▊       | 14/50 [00:05<00:12,  2.84it/s]\u001b[A\n",
            "\n",
            " 30%|███       | 15/50 [00:05<00:12,  2.84it/s]\u001b[A\n",
            "\n",
            " 32%|███▏      | 16/50 [00:05<00:11,  2.85it/s]\u001b[A\n",
            "\n",
            " 34%|███▍      | 17/50 [00:06<00:11,  2.85it/s]\u001b[A\n",
            "\n",
            " 36%|███▌      | 18/50 [00:06<00:11,  2.85it/s]\u001b[A\n",
            "\n",
            " 38%|███▊      | 19/50 [00:06<00:10,  2.85it/s]\u001b[A\n",
            "\n",
            " 40%|████      | 20/50 [00:07<00:10,  2.85it/s]\u001b[A\n",
            "\n",
            " 42%|████▏     | 21/50 [00:07<00:10,  2.85it/s]\u001b[A\n",
            "\n",
            " 44%|████▍     | 22/50 [00:07<00:09,  2.85it/s]\u001b[A\n",
            "\n",
            " 46%|████▌     | 23/50 [00:08<00:09,  2.85it/s]\u001b[A\n",
            "\n",
            " 48%|████▊     | 24/50 [00:08<00:09,  2.84it/s]\u001b[A\n",
            "\n",
            " 50%|█████     | 25/50 [00:08<00:08,  2.84it/s]\u001b[A\n",
            "\n",
            " 52%|█████▏    | 26/50 [00:09<00:08,  2.84it/s]\u001b[A\n",
            "\n",
            " 54%|█████▍    | 27/50 [00:09<00:08,  2.86it/s]\u001b[A\n",
            "\n",
            " 56%|█████▌    | 28/50 [00:09<00:07,  2.84it/s]\u001b[A\n",
            "\n",
            " 58%|█████▊    | 29/50 [00:10<00:07,  2.84it/s]\u001b[A\n",
            "\n",
            " 60%|██████    | 30/50 [00:10<00:07,  2.85it/s]\u001b[A\n",
            "\n",
            " 62%|██████▏   | 31/50 [00:11<00:06,  2.84it/s]\u001b[A\n",
            "\n",
            " 64%|██████▍   | 32/50 [00:11<00:06,  2.84it/s]\u001b[A\n",
            "\n",
            " 66%|██████▌   | 33/50 [00:11<00:05,  2.84it/s]\u001b[A\n",
            "\n",
            " 68%|██████▊   | 34/50 [00:12<00:05,  2.83it/s]\u001b[A\n",
            "\n",
            " 70%|███████   | 35/50 [00:12<00:05,  2.83it/s]\u001b[A\n",
            "\n",
            " 72%|███████▏  | 36/50 [00:12<00:04,  2.83it/s]\u001b[A\n",
            "\n",
            " 74%|███████▍  | 37/50 [00:13<00:04,  2.77it/s]\u001b[A\n",
            "\n",
            " 76%|███████▌  | 38/50 [00:13<00:04,  2.75it/s]\u001b[A\n",
            "\n",
            " 78%|███████▊  | 39/50 [00:13<00:04,  2.74it/s]\u001b[A\n",
            "\n",
            " 80%|████████  | 40/50 [00:14<00:03,  2.72it/s]\u001b[A\n",
            "\n",
            " 82%|████████▏ | 41/50 [00:14<00:03,  2.71it/s]\u001b[A\n",
            "\n",
            " 84%|████████▍ | 42/50 [00:15<00:02,  2.71it/s]\u001b[A\n",
            "\n",
            " 86%|████████▌ | 43/50 [00:15<00:02,  2.71it/s]\u001b[A\n",
            "\n",
            " 88%|████████▊ | 44/50 [00:15<00:02,  2.72it/s]\u001b[A\n",
            "\n",
            " 90%|█████████ | 45/50 [00:16<00:01,  2.71it/s]\u001b[A\n",
            "\n",
            " 92%|█████████▏| 46/50 [00:16<00:01,  2.69it/s]\u001b[A\n",
            "\n",
            " 94%|█████████▍| 47/50 [00:16<00:01,  2.67it/s]\u001b[A\n",
            "\n",
            " 96%|█████████▌| 48/50 [00:17<00:00,  2.70it/s]\u001b[A\n",
            "\n",
            " 98%|█████████▊| 49/50 [00:17<00:00,  2.74it/s]\u001b[A\n",
            "\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.76it/s]\u001b[A\n",
            "100%|██████████| 50/50 [00:17<00:00,  2.78it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.46s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.46s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "\n",
            "FID lower_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "FID lower_body : 100%|██████████| 1/1 [00:01<00:00,  1.68s/it]\n",
            "FID lower_body : 100%|██████████| 1/1 [00:02<00:00,  2.68s/it]\n",
            "\n",
            "KID lower_body :   0%|          | 0/1 [00:00<?, ?it/s]\n",
            "KID lower_body : 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\n",
            "KID lower_body : 100%|██████████| 1/1 [00:02<00:00,  2.39s/it]\n",
            "/usr/local/lib/python3.11/dist-packages/cleanfid/fid.py:78: RuntimeWarning: invalid value encountered in divide\n",
            "  t += (a.sum() - np.diag(a).sum()) / (m - 1) - b.sum() * 2 / m\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.11/dist-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.38it/s]\n",
            "100%|██████████| 1/1 [00:01<00:00,  1.37s/it]\n",
            "\n",
            "Image uploaded to Cloudinary. URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751130859/dkrgk05upj0l49ucz5qa.png\n",
            "✅ Result URL: https://res.cloudinary.com/dpl6zfv0y/image/upload/v1751130859/dkrgk05upj0l49ucz5qa.png\n",
            "✅ Inference done and result saved.\n",
            "🧹 Cleaned up temp files.\n",
            "[Keep-Alive] Preventing timeout...\n",
            "[Keep-Alive] Preventing timeout...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-06-28T17:16:14+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5000-1ec2d977-0eb7-43ae-985c-18205af020d1 acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        }
      ],
      "source": [
        "app.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyMe9gzh2F7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IdLdG1P2F4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-L9KVdsDzer"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6q_LQU2xs0Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}